# VLLM-based batch inference

- Currently only supports single-node inference (one model instance per node), but can be scaled to multiple instances by increasing `num_instances`
