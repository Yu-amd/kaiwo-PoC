# VLLM-based online inference

- Supports single-node and multi-node inference
- for best results, set `TENSOR_PARALLELISM` to number of GPUs per node and `PIPELINE_PARALLELISM` to number of nodes

