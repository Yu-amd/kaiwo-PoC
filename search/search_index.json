{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kaiwo","text":"<p>\ud83d\ude80\ufe0f\ud83d\ude80\ufe0f Kaiwo supports AMD GPUs! \ud83d\ude80\ufe0f\ud83d\ude80\ufe0f</p>"},{"location":"#description","title":"Description","text":"<p>Kaiwo (pronunciation \"ky-voh\") is a Kubernetes-native tool designed to optimize GPU resource utilization for AI workloads. Built on top of Ray and Kueue , Kaiwo minimizes GPU idleness and increases resource efficiency through intelligent job queueing, fair sharing of resources, guaranteed quotas and opportunistic gang scheduling.</p> <p>Kaiwo supports a wide range of AI workloads, including distributed multi-node pretraining, fine-tuning, online inference, and batch inference, with seamless integration into Kubernetes environments.</p> <p>This documentation is intended for two main audiences:</p> <ul> <li>AI Scientists/Engineers: who want Kaiwo to manage their AI workloads on Kubernetes. See here</li> <li>Infrastructure/Platform Administrators: who want to deploy and manage Kaiwo on their Kubernetes clusters. See here</li> </ul>"},{"location":"#main-features","title":"Main Features","text":"GPU Utilization Optimization Kaiwo Operator dynamically queues workloads to reduce GPU idle time and maximize resource utilization. CLI Tool Simplified workload submission using the kaiwo CLI tool Distributed Workload Scheduling Effortlessly schedule distributed workloads across multiple Kubernetes nodes with Kaiwo Operator. Broad Workload Support with pre-built templates Supports running Kubernetes Jobs, Deployments, RayJobs and RayServices. Integration with Ray and Kueue Leverages the power of Ray for distributed computing and Kueue for efficient job queueing."},{"location":"admin/auth/","title":"Authentication &amp; Authorization","text":"<p>Kaiwo integrates with Kubernetes authentication and authorization mechanisms.</p>"},{"location":"admin/auth/#cluster-queue-namespaces","title":"Cluster queue namespaces","text":"<p>Since the <code>KaiwoQueueConfig</code> lists the namespaces for the cluster queues, you must make sure that users that use these queues have the correct RBAC rights for these namespaces.</p>"},{"location":"admin/auth/#user-authentication-cli","title":"User Authentication (CLI)","text":"<p>If you wish to set up authentication on the Kube API server, please follow the official documentation. For setting up the <code>kaiwo</code> CLI to use this authentication, please see the CLI documentation.</p>"},{"location":"admin/auth/#authorization-rbac","title":"Authorization (RBAC)","text":"<p>Access control within Kubernetes, including permissions to create, view, and manage Kaiwo resources (<code>KaiwoJob</code>, <code>KaiwoService</code>, etc.) and underlying resources (Pods, Jobs, Deployments, PVCs), is managed through Kubernetes Role-Based Access Control (RBAC).</p> <ul> <li>Kaiwo Operator Permissions: The <code>install.yaml</code> manifest includes <code>ClusterRole</code> and <code>ClusterRoleBinding</code> (or <code>Role</code>/<code>RoleBinding</code>) definitions granting the Kaiwo operator's Service Account the necessary permissions to manage resources across the cluster or within specific namespaces. These permissions include creating/updating/deleting Jobs, Deployments, Ray resources, Kueue resources, PVCs, ConfigMaps, etc. Review the RBAC rules in <code>config/rbac/</code> in the source repository for specifics.</li> <li>User Permissions: As an administrator, you need to grant users appropriate RBAC permissions to interact with Kaiwo resources. Users typically need:<ul> <li><code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> permissions on <code>kaiwojobs.kaiwo.silogen.ai</code> and <code>kaiwoservices.kaiwo.silogen.ai</code> within their target namespaces.</li> <li><code>get</code>, <code>list</code>, <code>watch</code> permissions on Pods, Services, Events within their namespaces to allow <code>kaiwo manage</code>, <code>logs</code>, <code>monitor</code>, <code>exec</code>.</li> <li>Permissions to create <code>PersistentVolumeClaims</code> if they use the <code>storage</code> feature.</li> <li>Permissions to access <code>Secrets</code> referenced in their manifests (e.g., for image pulls, data download credentials).</li> </ul> </li> </ul> <p>Example User Role:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: ai-project-ns # Target namespace for the user\n  name: kaiwo-scientist-role\nrules:\n- apiGroups: [\"kaiwo.silogen.ai\"]\n  resources: [\"kaiwojobs\", \"kaiwoservices\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"\"] # Core API group\n  resources: [\"pods\", \"pods/log\", \"services\", \"events\", \"persistentvolumeclaims\", \"configmaps\", \"secrets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"] # For kaiwo exec/monitor\n  verbs: [\"create\"]\n- apiGroups: [\"batch\"]\n  resources: [\"jobs\"] # To view underlying batch jobs\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"] # To view underlying deployments\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"ray.io\"] # If using Ray\n  resources: [\"rayjobs\", \"rayservices\", \"rayclusters\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# Add other permissions as needed (e.g., create PVCs)\n</code></pre> <p>Bind this role to specific users or groups using a <code>RoleBinding</code>.</p>"},{"location":"admin/configuration/","title":"Configuration Guide","text":"<p>Administrators configure Kaiwo primarily through</p> <ul> <li>The cluster-scoped <code>KaiwoQueueConfig</code> Custom Resource Definition (CRD)</li> <li>The cluster-scoped <code>KaiwoConfig</code> CRD</li> <li>Environment variables or flags passed to the Kaiwo operator</li> </ul> <p>Users configure the CLI tool separately.</p>"},{"location":"admin/configuration/#kaiwoqueueconfig-crd","title":"KaiwoQueueConfig CRD","text":"<p>This is the central point for managing how Kaiwo interacts with Kueue. There can be only one <code>KaiwoQueueConfig</code> resource in the cluster, and its <code>metadata.name</code> must be <code>kaiwo</code> (or the value specified in the KaiwoConfig Custom Resource field <code>spec.defaultKaiwoQueueConfigName</code>, which defaults to <code>kaiwo</code>, see more below).</p> <p>Default Configuration on Startup:</p> <p>The Kaiwo operator includes a startup routine that checks if a <code>KaiwoQueueConfig</code> named <code>kaiwo</code> exists. If it does not, the operator automatically creates a default one. This default configuration aims to provide a functional baseline:</p> <ul> <li>It attempts to auto-discover node pools based on common GPU labels (e.g., <code>amd.com/gpu.product-name</code>, <code>nvidia.com/gpu.product</code>, <code>nvidia.com/gpu.count</code>) and CPU/Memory capacity.</li> <li>It creates corresponding Kueue <code>ResourceFlavor</code> resources based on this discovery, labeling the nodes with <code>kaiwo/nodepool=&lt;generated-flavor-name&gt;</code>.</li> <li>It defines a single Kueue <code>ClusterQueue</code> named <code>kaiwo</code> (or the value of <code>DEFAULT_CLUSTER_QUEUE_NAME</code>), configured to use all discovered <code>ResourceFlavors</code> and their estimated capacities as <code>nominalQuota</code>.</li> <li>It specifies that this default <code>ClusterQueue</code> should have a corresponding <code>LocalQueue</code> automatically created in the <code>kaiwo</code> namespace.</li> <li>It does not define any <code>WorkloadPriorityClass</code> resources by default.</li> </ul> <p>You can modify this automatically created configuration or create your own <code>kaiwo</code> resource manually using: <code>kubectl edit kaiwoqueueconfig kaiwo</code> or by applying a YAML manifest.</p> <p>Key Fields (<code>spec</code>):</p> <ul> <li> <p><code>resourceFlavors</code>: Defines the types of hardware resources available in the cluster, corresponding to Kueue <code>ResourceFlavor</code> resources.</p> <ul> <li><code>name</code>: A unique name for the flavor (e.g., <code>amd-mi300-8gpu</code>, <code>nvidia-a100-40gb</code>, <code>cpu-standard</code>).</li> <li><code>nodeLabels</code>: A map of labels that nodes must possess to be considered part of this flavor. This is crucial for scheduling pods onto the correct hardware. Example: <code>{\"kaiwo/nodepool\": \"amd-mi300-nodes\"}</code>.</li> <li><code>taints</code>: (Optional) A list of Kubernetes taints associated with this flavor. Pods scheduled to this flavor will need corresponding tolerations. Kaiwo automatically adds tolerations for GPU taints if <code>ADD_TAINTS_TO_GPU_NODES</code> is enabled.</li> </ul> <p>Auto-Discovery vs. Explicit Definition</p> <p>If <code>spec.resourceFlavors</code> is empty or omitted in the <code>kaiwo</code> <code>KaiwoQueueConfig</code>, the operator's startup logic attempts to auto-discover node pools and create corresponding flavors as described above. While convenient for initial setup, explicitly defining <code>resourceFlavors</code> in the <code>KaiwoQueueConfig</code> provides more precise control and is generally recommended for production environments. Explicitly defined flavors will override any auto-discovered ones during reconciliation.</p> </li> <li> <p><code>clusterQueues</code>: Defines the Kueue <code>ClusterQueue</code> resources managed by Kaiwo.</p> <ul> <li><code>name</code>: The name of the <code>ClusterQueue</code> (e.g., <code>team-a-queue</code>, <code>default-gpu-queue</code>).</li> <li><code>spec</code>: The full Kueue <code>ClusterQueueSpec</code>. This is where you define resource quotas, cohorts, preemption policies, etc. See Kueue ClusterQueue Documentation.<ul> <li><code>resourceGroups</code>: Define sets of flavors and their associated quotas (<code>nominalQuota</code>). This links the queue to the available hardware defined in <code>resourceFlavors</code>.</li> <li><code>namespaceSelector</code>: Controls which namespaces can use this queue via <code>LocalQueue</code> resources if those LocalQueues exist. Note that Kaiwo's automatic <code>LocalQueue</code> creation relies on the <code>namespaces</code> field below, not this selector.</li> </ul> </li> <li><code>namespaces</code>: A list of namespace names where Kaiwo should automatically create and manage a Kueue <code>LocalQueue</code> pointing to this <code>ClusterQueue</code>. The <code>LocalQueue</code> created will have the same name as the <code>ClusterQueue</code>.</li> </ul> </li> <li> <p><code>workloadPriorityClasses</code>: Defines Kueue <code>WorkloadPriorityClass</code> resources.</p> <ul> <li>Follows the standard Kueue <code>WorkloadPriorityClass</code> structure (<code>name</code>, <code>value</code>, <code>description</code>). Kaiwo ensures these exist as defined. See Kueue Priority Documentation.</li> </ul> </li> </ul> <p>Example <code>KaiwoQueueConfig</code>:</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoQueueConfig\nmetadata:\n  name: kaiwo # Must be named 'kaiwo' (or DEFAULT_KAIWO_QUEUE_CONFIG_NAME)\nspec:\n  resourceFlavors:\n    - name: amd-mi300-8gpu\n      nodeLabels:\n        kaiwo/nodepool: amd-mi300-nodes # Nodes with this label belong to this flavor\n        # Add other identifying labels if needed, e.g., topology.amd.com/gpu-count: '8'\n      # taints: # Optional, define if specific taints apply ONLY to these nodes\n      # - key: \"amd.com/gpu\"\n      #   operator: \"Exists\"\n      #   effect: \"NoSchedule\"\n    - name: cpu-high-mem\n      nodeLabels:\n        kaiwo/nodepool: cpu-high-mem-nodes\n\n  clusterQueues:\n    - name: ai-research-queue # Name of the ClusterQueue\n      namespaces: # Auto-create/manage LocalQueues in these namespaces\n        - ai-research-ns-1\n        - ai-research-ns-2\n      spec: # Standard Kueue ClusterQueueSpec\n        queueingStrategy: BestEffortFIFO\n        resourceGroups:\n          - coveredResources: [\"cpu\", \"memory\", \"amd.com/gpu\"] # Resources managed by this group\n            flavors:\n              - name: amd-mi300-8gpu # Reference to a defined resourceFlavor\n                resources:\n                  - name: \"cpu\"\n                    nominalQuota: \"192\" # Total CPU quota for this flavor in this queue\n                  - name: \"memory\"\n                    nominalQuota: \"1024Gi\" # Total Memory quota\n                  - name: \"amd.com/gpu\"\n                    nominalQuota: \"8\" # Total GPU quota\n          - coveredResources: [\"cpu\", \"memory\"]\n            flavors:\n              - name: cpu-high-mem\n                resources:\n                  - name: \"cpu\"\n                    nominalQuota: \"256\"\n                  - name: \"memory\"\n                    nominalQuota: \"2048Gi\"\n        # cohort: \"gpu-cohort\" # Optional: Group queues for borrowing/preemption\n        # preemption: ...\n\n  workloadPriorityClasses:\n    - name: high-priority\n      value: 1000\n    - name: low-priority\n      value: 100\n</code></pre>"},{"location":"admin/configuration/#controller-operation-and-kueue-resource-synchronization","title":"Controller Operation and Kueue Resource Synchronization","text":"<p>The <code>KaiwoQueueConfigController</code> acts as a translator, continuously ensuring that the Kueue resources in your cluster accurately reflect the configuration defined in the single <code>kaiwo</code> <code>KaiwoQueueConfig</code> resource. It monitors this resource and automatically manages the lifecycle of the associated Kueue objects:</p> <ul> <li> <p><code>spec.resourceFlavors</code> -&gt; Kueue <code>ResourceFlavor</code>:</p> <ul> <li>Each entry in this list directly defines a Kueue <code>ResourceFlavor</code>.</li> <li>The controller ensures a corresponding <code>ResourceFlavor</code> exists for each entry, creating or updating it as necessary based on the specified <code>name</code>, <code>nodeLabels</code>, and <code>taints</code>.</li> <li>If an entry is removed from this list, the controller deletes the corresponding <code>ResourceFlavor</code>.</li> </ul> </li> <li> <p><code>spec.clusterQueues</code> -&gt; Kueue <code>ClusterQueue</code> and <code>LocalQueue</code>:</p> <ul> <li>Each entry in this list defines a Kueue <code>ClusterQueue</code>. The controller translates the structure into a standard <code>ClusterQueueSpec</code> and ensures the resource exists and matches the definition. Removing an entry deletes the corresponding <code>ClusterQueue</code>.</li> <li>The <code>namespaces</code> field within each <code>clusterQueues</code> entry dictates where Kueue <code>LocalQueue</code>s should exist. The controller automatically creates a <code>LocalQueue</code> (named after the <code>ClusterQueue</code>) in each listed namespace, pointing to the corresponding <code>ClusterQueue</code>. If a namespace is removed from the list, or the parent <code>ClusterQueue</code> entry is removed, the controller deletes the associated <code>LocalQueue</code> in that namespace.</li> </ul> </li> <li> <p><code>spec.workloadPriorityClasses</code> -&gt; Kueue <code>WorkloadPriorityClass</code>:</p> <ul> <li>Each entry defines a Kueue <code>WorkloadPriorityClass</code>.</li> <li>The controller manages these resources, ensuring they exist with the specified <code>name</code>, <code>value</code>, and <code>description</code>.</li> <li>Removing an entry results in the deletion of the corresponding <code>WorkloadPriorityClass</code>.</li> </ul> </li> </ul> <p>Owner References</p> <p>The controller establishes the <code>kaiwo</code> <code>KaiwoQueueConfig</code> as the owner of all the Kueue resources it creates. This linkage ensures that if the <code>KaiwoQueueConfig</code> is deleted, Kubernetes automatically cleans up all the managed Kueue resources (<code>ResourceFlavor</code>, <code>ClusterQueue</code>, <code>LocalQueue</code>, <code>WorkloadPriorityClass</code>).</p> <p>Kueue resource management</p> <p>Kaiwo takes ownership of Kueue <code>ResourceFlavor</code>, <code>ClusterQueue</code>, <code>LocalQueue</code> and <code>WorkloadPriorityClass</code> resources. This means that resources of these types that are created manually, i.e. not via the <code>KaiwoQueueConfig</code>, may be deleted by the Kaiwo Controller</p> <p>The controller updates the <code>status.status</code> field of the <code>KaiwoQueueConfig</code> resource (<code>Pending</code>, <code>Ready</code>, or <code>Failed</code>) to indicate the current state of synchronization between the desired configuration and the actual Kueue resources in the cluster. This continuous reconciliation keeps the Kueue setup aligned with the central <code>KaiwoQueueConfig</code>.</p>"},{"location":"admin/configuration/#kaiwoconfig-crd","title":"KaiwoConfig CRD","text":"<p>The Kaiwo Operator's runtime configuration is managed through the <code>KaiwoConfig</code> Custom Resource Definition (CRD). This approach allows Kubernetes administrators to dynamically adjust operator behavior without requiring a restart. The operator always retrieves the most recent configuration values during each reconcile loop.</p>"},{"location":"admin/configuration/#configuration-structure","title":"Configuration Structure","text":"<p>The primary configuration resource is the <code>KaiwoConfig</code> CRD, typically maintained as a singleton within the Kubernetes cluster. Its key components are encapsulated in the <code>KaiwoConfigSpec</code>, which briefly includes:</p> <ul> <li><code>kueue</code>: Configures default integration settings with Kueue, including the default cluster queue name.</li> <li><code>ray</code>: Specifies Ray-specific parameters, including default container images and memory allocations.</li> <li><code>data</code>: Manages default filesystem paths for mounting data storage and HuggingFace caches.</li> <li><code>nodes</code>: Defines node-specific settings such as GPU resource keys, GPU node taints, and node pool exclusions.</li> <li><code>scheduling</code>: Sets scheduling-related configurations, like the Kubernetes scheduler name.</li> <li><code>resourceMonitoring</code>: Configures resource monitoring, including averaging intervals, utilization thresholds, and targeted namespaces.</li> <li><code>defaultKaiwoQueueConfigName</code>: Specifies the default name for the Kaiwo queue configuration object.</li> </ul>"},{"location":"admin/configuration/#specifying-the-configuration-cr","title":"Specifying the Configuration CR","text":"<p>The Kaiwo Operator identifies its configuration resource via the environment variable <code>CONFIG_NAME</code>. By default, this is set to <code>kaiwo</code>. Ensure that a <code>KaiwoConfig</code> resource with this exact name exists in your cluster. The operator automatically creates a default configuration at startup if none exists.</p> <p>Note</p> <p>The operator waits up to 30 seconds for the specified configuration resource to be found. If no resource is detected within this period, the operator pod will fail with an error.</p>"},{"location":"admin/configuration/#example-kaiwoconfig-cr","title":"Example KaiwoConfig CR","text":"<p>Here's a minimal example of a valid <code>KaiwoConfig</code> definition:</p> <pre><code>apiVersion: config.kaiwo.silogen.ai/v1alpha1\nkind: KaiwoConfig\nmetadata:\n  name: kaiwo\nspec:\n  scheduling:\n    kubeSchedulerName: \"kaiwo-scheduler\"\n  resourceMonitoring:\n    averagingTime: \"20m\"\n    lowUtilizationThreshold: 20\n    profile: \"gpu\"\n</code></pre> <p>For detailed descriptions of individual configuration fields, please see the full API reference.</p>"},{"location":"admin/configuration/#operator-environmental-variables","title":"Operator Environmental Variables","text":"<p>Some configuration is not changeable during runtime, or they may be often referenced from other config maps or secrets, and thus they are stored as environmental variables. These settings are not dynamic, and the operator must be restarted in order for changes to these values to take effect.</p>"},{"location":"admin/configuration/#kueue","title":"Kueue","text":"<ul> <li><code>DEFAULT_KAIWO_QUEUE_CONFIG_NAME</code>: The name of the singleton <code>KaiwoQueueConfig</code> custom resource to be used (defaults to <code>kaiwo</code>)</li> <li><code>DEFAULT_CLUSTER_QUEUE_NAME</code>: The name of the default Kueue cluster queue (defaults to <code>kaiwo</code>)</li> </ul>"},{"location":"admin/configuration/#resource-monitoring","title":"Resource Monitoring","text":"<p>To enable and configure resource monitoring within the Kaiwo Operator, the following environment variables must be set on the operator deployment:</p> <ul> <li><code>RESOURCE_MONITORING_ENABLED=true</code> \u2013 Enables the resource monitoring component.</li> <li><code>RESOURCE_MONITORING_PROMETHEUS_ENDPOINT=&lt;prometheus-endpoint&gt;</code> \u2013 Specifies the Prometheus endpoint to query metrics from.</li> <li><code>RESOURCE_MONITORING_POLLING_INTERVAL=10m</code> \u2013 Sets the interval between metric polling queries.</li> </ul>"},{"location":"admin/configuration/#other-configuration-options","title":"Other configuration options","text":"<ul> <li><code>WEBHOOK_CERT_DIRECTORY</code>: Path to manually provided webhook certificates (overrides automatic management if set). See Installation.</li> </ul> <p>Forthcoming feature</p> <p><code>ENFORCE_KAIWO_ON_GPU_WORKLOADS</code> (Default: <code>false</code>): If <code>true</code>, the mutating admission webhook for <code>batchv1.Job</code> will automatically add the <code>kaiwo.silogen.ai/managed: \"true\"</code> label to any job requesting GPU resources, forcing it to be managed by Kaiwo/Kueue.</p> <p>All environmental variables are typically set in the operator's <code>Deployment</code> manifest.</p> <p>Command-Line Flags:</p> <p>Refer to the output of <code>kaiwo-operator --help</code> (or check <code>cmd/operator/main.go</code>) for flags controlling metrics, health probes, leader election, and certificate paths.</p>"},{"location":"admin/installation/","title":"Installation Guide","text":"<p>This guide provides detailed steps for installing the Kaiwo operator and its dependencies on a Kubernetes cluster.</p>"},{"location":"admin/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (v1.22+ recommended).</li> <li><code>kubectl</code> installed and configured with cluster-admin privileges.</li> <li><code>git</code> (if cloning repositories).</li> <li>Go (if using Cluster Forge).</li> </ul>"},{"location":"admin/installation/#dependency-overview","title":"Dependency Overview","text":"<p>Kaiwo requires several core Kubernetes components to function correctly.</p> <ol> <li>Cert-Manager: Manages TLS certificates for webhooks.</li> <li>GPU Operator:<ul> <li>NVIDIA: NVIDIA GPU Operator + GPU Feature Discovery.</li> <li>AMD: AMD GPU Operator. (Includes Node Labeller).</li> <li>Ensures GPU drivers are installed and nodes are correctly labeled with GPU information.</li> </ul> </li> <li>Kueue: Provides job queueing, fair sharing, and quota management. (Docs).</li> <li>KubeRay Operator: Required only if users will run Ray-based workloads (<code>spec.ray: true</code>). Manages Ray clusters. (Docs).</li> <li>AppWrapper: Used by Kueue to manage atomic scheduling of complex workloads, particularly Ray clusters/services. (GitHub).</li> <li>Prometheus (Recommended): For monitoring the Kaiwo operator and cluster metrics.</li> </ol>"},{"location":"admin/installation/#step-1-install-kaiwo-and-its-dependencies","title":"Step 1: Install Kaiwo and its dependencies","text":"<p>There are several different ways that you can install the Kaiwo dependencies and operator. The following serve as references that you can adapt to your particular environment and workflow.</p>"},{"location":"admin/installation/#dependencies-via-convenience-script","title":"Dependencies via convenience script","text":"<p>You can install the dependencies using the convenience script:</p> <p>From the remote script</p> <pre><code>curl -sSL https://raw.githubusercontent.com/silogen/kaiwo/refs/heads/main/dependencies/install_dependencies.sh | bash -s --\n</code></pre> <p>Or if you have cloned the repository</p> <pre><code>bash dependencies/install_dependencies.sh --local\n</code></pre> <p>GPU Operator Not Included</p> <p>You must install the AMD GPU Operator separately according to its documentation before running the convenience script or installing Kaiwo. Ensure node labeling features are enabled.</p>"},{"location":"admin/installation/#kaiwo-operator-via-install-manifest","title":"Kaiwo operator via install manifest","text":"<p>Once dependencies are ready, install the Kaiwo operator itself.</p> <p>You can install the latest version via:</p> <pre><code>kubectl apply -f https://github.com/silogen/kaiwo/releases/latest/download/install.yaml --server-side\n</code></pre> <p>If you want to choose the release, follow these steps:</p> <ol> <li>Choose Release: Find the latest stable release tag on the Kaiwo GitHub Releases page.</li> <li> <p>Apply Manifest: Use <code>kubectl apply</code> with the <code>--server-side</code> flag (recommended for managing large manifests and CRDs). Replace <code>vX.Y.Z</code> with your chosen release tag.</p> <pre><code>export KAIWO_VERSION=vX.Y.Z\nkubectl apply -f https://github.com/silogen/kaiwo/releases/download/${KAIWO_VERSION}/install.yaml --server-side\n</code></pre> <p>This installs:</p> <ul> <li>Kaiwo CRDs (<code>KaiwoJob</code>, <code>KaiwoService</code>, <code>KaiwoQueueConfig</code>)</li> <li>The Kaiwo Controller Manager <code>Deployment</code> in the <code>kaiwo-system</code> namespace.</li> <li>RBAC rules (<code>ClusterRole</code>, <code>Role</code>, <code>ClusterRoleBinding</code>, <code>RoleBinding</code>).</li> <li>Webhook configurations (if enabled in the release).</li> <li>Service for webhooks/metrics.</li> </ul> </li> </ol>"},{"location":"admin/installation/#everything-via-cluster-forge","title":"Everything via Cluster Forge","text":"<p>Cluster Forge is a tool for managing Kubernetes stacks. You can use it to install Kaiwo and its dependencies.</p> <ol> <li>Clone the Cluster Forge repository: <code>git clone https://github.com/silogen/cluster-forge.git</code></li> <li>Navigate into the directory: <code>cd cluster-forge</code></li> <li>Ensure Go is installed (<code>go version</code>).</li> <li>Run the forge command, selecting <code>kaiwo-all</code> and optionally the relevant GPU operator (<code>amd-gpu-operator</code>):     <pre><code>go run . forge -s kaiwo\n# Follow prompts to select 'kaiwo-all' and your GPU operator stack.\n</code></pre></li> <li>Deploy the selected stack:     <pre><code>bash stacks/kaiwo/deploy.sh\n</code></pre></li> <li>Verify pods in relevant namespaces (<code>kaiwo-system</code>, <code>cert-manager</code>, <code>kueue-system</code>, etc.).</li> </ol>"},{"location":"admin/installation/#manually","title":"Manually","text":"<p>If you prefer to manage the dependencies yourself, you can inspect the <code>/dependencies</code> folder to see what is required, and install Kaiwo yourself by using the <code>install.yaml</code> release from the releases page.</p>"},{"location":"admin/installation/#step-2-verify-installation","title":"Step 2: Verify Installation","text":"<ol> <li> <p>Check Operator Pod: Ensure the Kaiwo controller manager pod is running.     <pre><code>kubectl get pods -n kaiwo-system -l control-plane=kaiwo-controller-manager\n# Example Output:\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# kaiwo-controller-manager-6c...-...            1/1     Running   0          2m\n</code></pre></p> </li> <li> <p>Check CRDs: Verify that the Kaiwo Custom Resource Definitions are installed.     <pre><code>kubectl get crds | grep kaiwo.silogen.ai\n# Example Output:\n# kaiwojoblists.kaiwo.silogen.ai          ...\n# kaiwojobs.kaiwo.silogen.ai              ...\n# kaiwoqueueconfigs.kaiwo.silogen.ai      ...\n# kaiwoservicelists.kaiwo.silogen.ai      ...\n# kaiwoservices.kaiwo.silogen.ai          ...\n</code></pre></p> </li> <li> <p>Check Default QueueConfig: The operator should automatically create a default <code>KaiwoQueueConfig</code>.     <pre><code>kubectl get kaiwoqueueconfig kaiwo\n# Example Output:\n# NAME    AGE\n# kaiwo   3m\n</code></pre>     If this is missing, check the operator logs: <code>kubectl logs -n kaiwo-system -l control-plane=kaiwo-controller-manager</code></p> </li> </ol>"},{"location":"admin/installation/#step-3-provide-kaiwo-cli-to-users","title":"Step 3: Provide Kaiwo CLI to Users","text":"<p>Instruct your users (AI Scientists/Engineers) on how to download and install the <code>kaiwo</code> CLI tool. Point them to the User Quickstart guide or the CLI Installation instructions.</p>"},{"location":"admin/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Kaiwo: Customize the default <code>KaiwoQueueConfig</code> (<code>kubectl edit kaiwoqueueconfig kaiwo</code>) to define appropriate Kueue <code>ResourceFlavors</code> and <code>ClusterQueues</code> reflecting your cluster's hardware and policies. See the Configuration Guide.</li> <li>Set up Monitoring: Integrate Kaiwo operator metrics with your monitoring system (e.g., Prometheus). See the Monitoring Guide.</li> <li>Authentication: Ensure users have the necessary <code>kubeconfig</code> files and any required authentication plugins installed. See Authentication &amp; Authorization.</li> </ul>"},{"location":"admin/maintenance/","title":"Maintenance Guide","text":"<p>This section outlines common maintenance tasks for the Kaiwo system.</p>"},{"location":"admin/maintenance/#upgrading-kaiwo","title":"Upgrading Kaiwo","text":"<ol> <li>Check Release Notes: Before upgrading, review the release notes for the target version on the GitHub Releases page. Pay attention to any breaking changes, dependency updates, or manual migration steps required.</li> <li>Backup (Optional but Recommended): Consider backing up relevant configurations, especially your <code>KaiwoQueueConfig</code> CRD (<code>kubectl get kaiwoqueueconfig kaiwo -o yaml &gt; kaiwoqueueconfig_backup.yaml</code>).</li> <li> <p>Apply New Manifest: Apply the <code>install.yaml</code> manifest for the new version using <code>kubectl apply --server-side</code>.</p> <p><pre><code>export KAIWO_NEW_VERSION=vX.Y.Z # Set to the target version\nkubectl apply -f https://github.com/silogen/kaiwo/releases/download/${KAIWO_NEW_VERSION}/install.yaml --server-side\n</code></pre> 4.  Verify Upgrade:</p> <ul> <li>Check that the Kaiwo operator pod restarts and uses the new image version:     <pre><code>kubectl get pods -n kaiwo-system -l control-plane=kaiwo-controller-manager\nkubectl describe pod -n kaiwo-system -l control-plane=kaiwo-controller-manager | grep Image:\n</code></pre></li> <li>Monitor operator logs for any errors during startup:     <pre><code>kubectl logs -n kaiwo-system -l control-plane=kaiwo-controller-manager -f\n</code></pre></li> <li>Ensure Kaiwo CRDs remain functional and workloads continue to be processed.</li> <li>Upgrade Dependencies: If the Kaiwo release notes indicate required upgrades for dependencies (Kueue, Ray, Cert-Manager, etc.), perform those upgrades according to their respective documentation.</li> </ul> </li> </ol>"},{"location":"admin/maintenance/#certificate-rotation","title":"Certificate Rotation","text":"<ul> <li>Cert-Manager (Default): If using the default setup with Cert-Manager, certificate rotation for webhooks is typically handled automatically based on the <code>Certificate</code> resources created during installation. Monitor Cert-Manager logs and certificate expiry (<code>kubectl get certificates -n kaiwo-system</code>) if issues arise.</li> </ul>"},{"location":"admin/maintenance/#operator-pod-management","title":"Operator Pod Management","text":"<ul> <li>Restarting: If the operator becomes unresponsive, you can restart it by deleting the pod:     <pre><code>kubectl delete pod -n kaiwo-system -l control-plane=kaiwo-controller-manager\n</code></pre>     The Deployment will automatically create a new pod.</li> <li>Scaling: The Kaiwo operator deployment typically runs with a single replica due to leader election (<code>--leader-elect=true</code>). Scaling is generally not required unless leader election is disabled (not recommended for production).</li> </ul>"},{"location":"admin/maintenance/#cleaning-up-resources","title":"Cleaning Up Resources","text":"<ul> <li>Workloads: Users can delete their workloads using <code>kaiwo manage</code> or <code>kubectl delete kaiwojob &lt;name&gt;</code> / <code>kubectl delete kaiwoservice &lt;name&gt;</code>. The Kaiwo operator's finalizers ensure associated resources (like underlying Jobs/Deployments, PVCs created by Kaiwo download jobs) are cleaned up.</li> <li>Kueue Resources: Resources managed by <code>KaiwoQueueConfig</code> (Flavors, ClusterQueues, PriorityClasses) are typically deleted if removed from the <code>kaiwo</code> <code>KaiwoQueueConfig</code> spec.</li> <li>Uninstalling Kaiwo: To completely remove Kaiwo, delete the installation manifest and clean up CRDs and namespaces:     <pre><code># Replace vX.Y.Z with the installed version\nexport KAIWO_VERSION=vX.Y.Z\nkubectl delete -f https://github.com/silogen/kaiwo/releases/download/${KAIWO_VERSION}/install.yaml\n\n# Delete CRDs (use with caution - this will delete ALL KaiwoJob/Service/QueueConfig resources)\nkubectl delete crd kaiwojobs.kaiwo.silogen.ai\nkubectl delete crd kaiwoservices.kaiwo.silogen.ai\nkubectl delete crd kaiwoqueueconfigs.kaiwo.silogen.ai\n# ... delete other Kaiwo CRDs if any\n\n# Delete namespace\nkubectl delete namespace kaiwo-system\n</code></pre>     Remember to also uninstall dependencies if they are no longer needed.</li> </ul>"},{"location":"admin/monitoring/","title":"Monitoring Kaiwo","text":"<p>Monitoring the Kaiwo operator and the workloads it manages is crucial for ensuring system health and performance.</p>"},{"location":"admin/monitoring/#operator-metrics","title":"Operator Metrics","text":"<p>The Kaiwo operator exposes metrics in Prometheus format.</p> <ul> <li>Endpoint: By default, metrics are exposed on port <code>8080</code> (HTTP) or <code>8443</code> (HTTPS, if <code>--metrics-secure=true</code>, which is the default). The bind address can be configured via the <code>--metrics-bind-address</code> flag (defaults to <code>0</code> which disables the endpoint unless overridden). The <code>install.yaml</code> manifest typically configures this.</li> <li>Security: When <code>--metrics-secure=true</code>, the endpoint uses TLS. Certificates can be auto-generated by controller-runtime, managed by Cert-Manager, or provided manually via flags (<code>--metrics-cert-path</code>, etc.). Authentication and authorization can be enabled via <code>controller-runtime</code>'s filters (<code>metricsServerOptions.FilterProvider = filters.WithAuthenticationAndAuthorization</code>). RBAC for accessing the metrics endpoint needs to be configured separately (see <code>config/rbac/kustomization.yaml</code> in the source repository for examples).</li> <li>Key Metrics: The operator exposes standard controller-runtime metrics (e.g., reconcile times, errors, queue lengths) and potentially custom metrics related to Kaiwo operations.</li> </ul> <p>Integration with Prometheus:</p> <ol> <li>Ensure a Prometheus instance (e.g., Prometheus Operator, managed Prometheus service) is running in your cluster.</li> <li>Configure Prometheus to scrape the Kaiwo operator's metrics endpoint. This typically involves creating a <code>ServiceMonitor</code> or <code>PodMonitor</code> resource targeting the <code>kaiwo-controller-manager</code> service/pods in the <code>kaiwo-system</code> namespace.</li> <li>If using TLS (<code>--metrics-secure=true</code>), configure Prometheus scraping job with the appropriate TLS configuration (e.g., <code>insecure_skip_verify: true</code> for self-signed certs, or proper CA/client certs).</li> </ol> <p>Consult the <code>controller-runtime</code> documentation and your Prometheus setup guide for detailed scraping configuration.</p>"},{"location":"admin/monitoring/#operator-logs","title":"Operator Logs","text":"<p>Monitor the logs of the Kaiwo operator pod for errors or important events:</p> <pre><code>kubectl logs -n kaiwo-system -l control-plane=kaiwo-controller-manager -f\n</code></pre> <p>Consider shipping these logs to a central logging system (e.g., Loki, Elasticsearch, Splunk) for easier analysis and alerting.</p>"},{"location":"admin/monitoring/#kueue-monitoring","title":"Kueue Monitoring","text":"<p>Kueue also exposes its own metrics and has status conditions on its resources (<code>ClusterQueue</code>, <code>LocalQueue</code>, <code>Workload</code>). Monitoring Kueue is essential for understanding queue lengths, resource utilization, admission decisions, and potential bottlenecks.</p> <ul> <li>Kueue Metrics: Scrape metrics from the <code>kueue-controller-manager</code> similar to the Kaiwo operator.</li> <li>Queue Status: Check the status of <code>ClusterQueue</code> and <code>LocalQueue</code> resources:     <pre><code>kubectl get clusterqueue &lt;queue-name&gt; -o yaml\nkubectl get localqueue -n &lt;namespace&gt; &lt;queue-name&gt; -o yaml\n</code></pre></li> <li>Workload Status: Inspect <code>Workload</code> resources created by Kueue for admitted/pending jobs:     <pre><code>kubectl get workloads -n &lt;namespace&gt;\nkubectl describe workload -n &lt;namespace&gt; &lt;workload-name&gt;\n</code></pre></li> </ul> <p>Refer to the Kueue documentation for details on its metrics.</p>"},{"location":"admin/monitoring/#workload-status-and-events","title":"Workload Status and Events","text":"<p>Monitor the status of the <code>KaiwoJob</code> and <code>KaiwoService</code> resources themselves:</p> <pre><code>kubectl get kaiwojobs -A\nkubectl get kaiwoservices -A\n\nkubectl describe kaiwojob -n &lt;namespace&gt; &lt;job-name&gt;\nkubectl describe kaiwoservice -n &lt;namespace&gt; &lt;service-name&gt;\n</code></pre> <p>Check the <code>status</code> field for the overall phase (<code>PENDING</code>, <code>RUNNING</code>, <code>COMPLETE</code>, <code>FAILED</code>, <code>READY</code>) and <code>conditions</code>.</p> <p>Also, monitor Kubernetes events related to Kaiwo resources and the underlying pods/jobs/deployments:</p> <pre><code>kubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"admin/monitoring/#cluster-resource-utilization","title":"Cluster Resource Utilization","text":"<p>Use standard Kubernetes monitoring tools (e.g., <code>kubectl top nodes</code>, <code>kubectl top pods</code>, Prometheus with <code>kube-state-metrics</code> and <code>node-exporter</code>) to track overall cluster CPU, memory, and GPU utilization. Pay special attention to GPU utilization on nodes designated for AI workloads.</p>"},{"location":"admin/monitoring/#dashboards-and-alerting","title":"Dashboards and Alerting","text":"<p>Create dashboards (e.g., in Grafana) combining metrics from the Kaiwo operator, Kueue, GPU operator, and standard Kubernetes components to get a holistic view of the system. Set up alerts based on key metrics or status conditions (e.g., high queue lengths, operator errors, low GPU utilization, failed workloads).</p>"},{"location":"admin/overview/","title":"Overview for Administrators","text":"<p>Kaiwo provides a layer on top of Kubernetes, Kueue, and Ray to streamline the management and execution of AI workloads, particularly focusing on efficient GPU utilization. As an administrator, your role involves deploying, configuring, and maintaining the Kaiwo system.</p>"},{"location":"admin/overview/#key-components-and-concepts","title":"Key Components and Concepts","text":"<ol> <li> <p>Kaiwo Operator:</p> <ul> <li>Runs as a deployment within the Kubernetes cluster.</li> <li>Manages the lifecycle of Kaiwo Custom Resources (<code>KaiwoJob</code>, <code>KaiwoService</code>, <code>KaiwoQueueConfig</code>).</li> <li>Controllers: Includes specific controllers for each CRD:<ul> <li><code>KaiwoJobController</code>: Translates <code>KaiwoJob</code> into <code>batchv1.Job</code> or <code>rayv1.RayJob</code>, manages dependencies (like download jobs, PVCs), and updates status.</li> <li><code>KaiwoServiceController</code>: Translates <code>KaiwoService</code> into <code>appsv1.Deployment</code> or <code>rayv1.RayService</code> (wrapped in an <code>AppWrapper</code>), manages dependencies, and updates status.</li> <li><code>KaiwoQueueConfigController</code>: Manages Kueue resources (<code>ClusterQueue</code>, <code>ResourceFlavor</code>, <code>WorkloadPriorityClass</code>) based on the cluster-scoped <code>KaiwoQueueConfig</code> CRD. Ensures a default configuration exists.</li> </ul> </li> <li>Integration: Interacts with the Kubernetes API, Kueue, and Ray operators.</li> </ul> </li> <li> <p>Kaiwo CRDs:</p> <ul> <li><code>KaiwoJob</code> / <code>KaiwoService</code>: User-facing resources defined by AI Scientists to describe their workloads. They abstract away much of the underlying Kubernetes/Ray/Kueue complexity.</li> <li><code>KaiwoQueueConfig</code>: A cluster-scoped resource (typically one named <code>kaiwo</code>) used by administrators to define and manage Kueue configurations centrally. This includes defining queues, resource types (flavors), and priorities.</li> </ul> </li> <li> <p>Kueue Integration:</p> <ul> <li>Kaiwo relies on Kueue for job queueing, scheduling, and resource quota management.</li> <li>The Kaiwo Operator, specifically the <code>KaiwoQueueConfigController</code>, manages the creation and synchronization of Kueue <code>ClusterQueue</code>, <code>ResourceFlavor</code>, and <code>WorkloadPriorityClass</code> resources based on the <code>KaiwoQueueConfig</code> CRD.</li> <li>Workloads (<code>KaiwoJob</code>/<code>KaiwoService</code>) are submitted to a specific <code>ClusterQueue</code> (via the <code>kueue.x-k8s.io/queue-name</code> label, derived from <code>spec.clusterQueue</code>).</li> </ul> </li> <li> <p>Ray Integration:</p> <ul> <li>If <code>spec.ray: true</code> is set in a <code>KaiwoJob</code> or <code>KaiwoService</code>, the operator creates <code>RayJob</code> or <code>RayService</code> resources instead of standard Kubernetes ones.</li> <li>This leverages Ray for distributed execution capabilities. Requires the KubeRay operator to be installed.</li> </ul> </li> <li> <p>Kaiwo CLI:</p> <ul> <li>The primary user interface for AI Scientists.</li> <li>Communicates with the Kubernetes API to create and manage Kaiwo CRDs.</li> <li>Requires <code>kubeconfig</code> access similar to <code>kubectl</code>.</li> </ul> </li> </ol>"},{"location":"admin/overview/#administrator-responsibilities","title":"Administrator Responsibilities","text":"<ul> <li>Installation: Deploying the Kaiwo operator and its dependencies (Kueue, Ray Operator, Cert-Manager, GPU Operator, etc.).</li> <li>Configuration: Defining cluster-wide queuing policies, resource flavors (mapping to node types/pools), and priorities using the <code>KaiwoQueueConfig</code> CRD. Managing storage classes referenced by users.</li> <li>Maintenance: Upgrading Kaiwo components, monitoring operator health, managing certificates.</li> <li>Monitoring: Observing cluster resource utilization, queue lengths, and workload statuses. Integrating with monitoring tools like Prometheus.</li> <li>User Management: Potentially managing namespaces and ensuring users target appropriate Kueue queues.</li> <li>Troubleshooting: Diagnosing issues related to scheduling, resource allocation, operator errors, or workload failures.</li> </ul>"},{"location":"admin/resource-monitoring/","title":"Resource Monitoring","text":"<p>The Kaiwo Operator includes a resource monitoring utility which continuously watches your Kaiwo workloads (Jobs or Services) and checks their GPU utilization via metrics endpoints. If any pod of a workload that reserves GPUs is underutilizing the GPU, the operator marks the workload as Underutilized and emits an event. If the workload does not utilize the GPU for a given amount of time, it is automatically terminated. This termination feature is enabled by default if resource monitoring is enabled, but it can be disabled in case you want to implement your own termination logic.</p> <p>In order for workloads to be monitored, they must be deployed via Kaiwo CRDs (<code>KaiwoJob</code> or <code>KaiwoService</code>). This ensures that the created resources have the correct labels and are inspected by the resource monitor.</p>"},{"location":"admin/resource-monitoring/#configuration","title":"Configuration","text":""},{"location":"admin/resource-monitoring/#operator-environmental-variables","title":"Operator Environmental Variables","text":"<p>Resource monitoring is enabled via environmental variables given to the Kaiwo operator: </p> Parameter Description Default <code>RESOURCE_MONITORING_ENABLED</code> Enable or disable monitoring (<code>true</code>/<code>false</code>) <code>false</code> <code>RESOURCE_MONITORING_METRICS_ENDPOINT</code> URL of your metrics endpoint (required) <code>RESOURCE_MONITORING_POLLING_INTERVAL</code> How often to check metrics (e.g. <code>30s</code>, <code>1m</code>) (required) <p>Note</p> <p>Setting the polling interval very long with workloads that only use GPUs occasionally may end up causing false early terminations, if the GPU is not in use during the polling check. Ensure that your polling interval is low enough to catch GPU usage based on your workload.</p> <p>These options are set as the operator environment variables and cannot be changed during runtime.</p>"},{"location":"admin/resource-monitoring/#resourcemonitoring-field-in-kaiwoconfig","title":"<code>resourceMonitoring</code> field in KaiwoConfig","text":"<p>Please see the CRD documentation for the available options for setting the runtime configuration for the resource monitoring. Changing these fields takes effect immediately.</p> <p>By default, <code>terminateUnderutilizingAfter</code> is set to 24 hours and <code>lowUtilizationThreshold</code> is set to 1 (percent). This means that if a workload reaches at least 1 % GPU utilization at least once over 24 hours, it will not be terminated. These values should most likely be changed to suit your environment.</p>"},{"location":"admin/resource-monitoring/#terminating-underutilizing-workloads","title":"Terminating Underutilizing Workloads","text":"<p>If the KaiwoConfig field <code>spec.resourceMonitoring.terminateUnderutilizing</code> is <code>true</code> (the default), once a workload has been underutilizing one or more GPUs continuously for the time specified in the field <code>spec.resourceMonitoring.terminateUnderutilizingAfter</code>, it is flagged for termination by setting the early termination condition and setting the status to <code>TERMINATING</code>. The Kaiwo operator will then take care of deleting the dependent resources, but keeps the Kaiwo workload object available to provide a way to inspect the reason for termination.</p>"},{"location":"admin/resource-monitoring/#status-conditions","title":"Status Conditions","text":"<p>Once monitoring begins, each KaiwoWorkload will have a condition under <code>.status.conditions</code>:</p> <pre><code>- type: ResourceUnderutilization\n  status: \"False\"    # \u201cTrue\u201d means Underutilized, \u201cFalse\u201d means Normal\n  reason: GpuUtilizationNormal    # or GpuUtilizationLow\n  message: \"GPU utilization normal\"\n</code></pre> <p>If a workload is flagged for early termination, it will have an additional condition:</p> <pre><code>- type: WorkloadTerminatedEarly\n  status: \"True\"\n  reason: GpuUtilizationLow    # or GpuUtilizationLow\n  message: \"Early termination due to low GPU usage\"\n</code></pre>"},{"location":"admin/resource-monitoring/#best-practices","title":"Best Practices","text":"<ul> <li>Right-size your thresholds   Choose a sensible cutoff (e.g. 10\u201330%) so you catch idle pods without false positives</li> <li>Namespace filtering   Use the KaiwoConfig field <code>spec.resourceMonitoring.targetNamespaces</code> to restrict monitoring to critical workloads only.</li> </ul>"},{"location":"admin/resource-monitoring/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No status updates?<ul> <li>Ensure <code>ENABLED=true</code> and <code>METRICS_ENDPOINT</code> is reachable.</li> <li>Check operator logs for query errors.</li> </ul> </li> <li>Excessive events?<ul> <li>Increase <code>lowUtilizationThreshold</code> to reduce sensitivity.</li> </ul> </li> </ul>"},{"location":"admin/troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide provides steps for diagnosing common issues with Kaiwo.</p>"},{"location":"admin/troubleshooting/#operator-issues","title":"Operator Issues","text":""},{"location":"admin/troubleshooting/#operator-pod-not-running-or-crashing","title":"Operator Pod Not Running or Crashing","text":"<ol> <li> <p>Check Pod Status:     <pre><code>kubectl get pods -n kaiwo-system -l control-plane=kaiwo-controller-manager\n</code></pre>     Look for pods in <code>CrashLoopBackOff</code>, <code>Error</code>, or <code>Pending</code> states.</p> </li> <li> <p>Examine Pod Logs:     <pre><code>kubectl logs -n kaiwo-system -l control-plane=kaiwo-controller-manager\n# Or for a specific crashing pod:\nkubectl logs -n kaiwo-system &lt;pod-name&gt; -p # Check previous container logs\n</code></pre>     Look for error messages related to startup, configuration, API connectivity, or reconciliation loops.</p> </li> <li> <p>Describe Pod:     <pre><code>kubectl describe pod -n kaiwo-system &lt;pod-name&gt;\n</code></pre>     Check for events related to scheduling failures (resource constraints, taints/tolerations), image pull errors, readiness/liveness probe failures, or volume mount issues.</p> </li> <li> <p>Check Dependencies: Ensure all dependencies (Cert-Manager, Kueue, Ray Operator, GPU Operator, AppWrapper) are running correctly in their respective namespaces. Check their logs if necessary.</p> </li> <li> <p>RBAC Permissions: Verify the Kaiwo operator's <code>ServiceAccount</code>, <code>ClusterRole</code>, and <code>ClusterRoleBinding</code> grant sufficient permissions. Errors related to \"forbidden\" access often point to RBAC issues.</p> </li> <li> <p>Webhook Issues: If webhooks are enabled, check Cert-Manager status and webhook service connectivity. Invalid certificates or network policies blocking webhook calls can prevent resource creation/updates.</p> <ul> <li>Check webhook configurations: <code>kubectl get mutatingwebhookconfigurations</code>, <code>kubectl get validatingwebhookconfigurations</code></li> <li>Check certificate status: <code>kubectl get certificates -n kaiwo-system</code></li> <li>Test webhook service endpoint.</li> </ul> </li> </ol>"},{"location":"admin/troubleshooting/#default-kaiwoqueueconfig-not-created","title":"Default <code>KaiwoQueueConfig</code> Not Created","text":"<ul> <li>Check operator logs (<code>kubectl logs -n kaiwo-system -l control-plane=kaiwo-controller-manager</code>) for errors during the startup routine that creates the default configuration.</li> <li>Common causes include inability to list Nodes (RBAC issue) or errors during node labeling/tainting if enabled.</li> </ul>"},{"location":"admin/troubleshooting/#kueue-resources-not-syncing","title":"Kueue Resources Not Syncing","text":"<ul> <li>Ensure the <code>kaiwo</code> <code>KaiwoQueueConfig</code> resource exists (<code>kubectl get kaiwoqueueconfig kaiwo</code>).</li> <li>Check operator logs for errors related to creating/updating Kueue <code>ResourceFlavors</code>, <code>ClusterQueues</code>, or <code>WorkloadPriorityClasses</code>.</li> <li>Verify the operator has RBAC permissions to manage these Kueue resources.</li> <li>Check Kueue controller logs (<code>kubectl logs -n kueue-system -l control-plane=controller-manager -f</code>) for related errors.</li> </ul>"},{"location":"admin/troubleshooting/#workload-issues","title":"Workload Issues","text":""},{"location":"admin/troubleshooting/#workload-stuck-in-pending","title":"Workload Stuck in <code>PENDING</code>","text":"<p>This usually means Kueue has not admitted the workload yet.</p> <ol> <li> <p>Check Kueue Workload Status: Find the Kueue <code>Workload</code> resource corresponding to your <code>KaiwoJob</code>/<code>KaiwoService</code>.     <pre><code># Find the workload (often named after the Kaiwo resource)\nkubectl get workloads -n &lt;namespace&gt;\n# Describe the workload to see admission status and reasons for pending\nkubectl describe workload -n &lt;namespace&gt; &lt;workload-name&gt;\n</code></pre>     Look for conditions like <code>Admitted</code> being <code>False</code> and check the <code>Message</code> for reasons (e.g., quota exhaustion, no matching <code>ResourceFlavor</code>).</p> </li> <li> <p>Check ClusterQueue Status:     <pre><code>kubectl describe clusterqueue &lt;queue-name&gt;\n</code></pre>     Look at usage vs. quota (<code>nominalQuota</code>) for relevant resource flavors.</p> </li> <li> <p>Check ResourceFlavor Definitions: Ensure <code>ResourceFlavors</code> defined in <code>KaiwoQueueConfig</code> correctly match node labels in your cluster.</p> </li> <li> <p>Check LocalQueue: Ensure a <code>LocalQueue</code> pointing to the correct <code>ClusterQueue</code> exists in the workload's namespace (<code>kubectl get localqueue -n &lt;namespace&gt; &lt;queue-name&gt;</code>). Kaiwo operator should create these if specified in <code>KaiwoQueueConfig.spec.clusterQueues[].namespaces</code>.</p> </li> </ol>"},{"location":"admin/troubleshooting/#workload-fails-immediately-status-failed","title":"Workload Fails Immediately (Status <code>FAILED</code>)","text":"<ol> <li> <p>Check Kaiwo Resource Events:     <pre><code>kubectl describe kaiwojob -n &lt;namespace&gt; &lt;job-name&gt;\n# or\nkubectl describe kaiwoservice -n &lt;namespace&gt; &lt;service-name&gt;\n</code></pre>     Look for events indicating failures during dependency creation (e.g., PVC, download job) or underlying resource creation.</p> </li> <li> <p>Check Download Job Logs (if applicable): If using <code>spec.storage</code> with downloads, check the logs of the downloader job pod.     <pre><code># Find the downloader pod (usually name ends with '-download-&lt;hash&gt;')\nkubectl get pods -n &lt;namespace&gt; | grep &lt;job-name&gt;-download\n# Get logs\nkubectl logs -n &lt;namespace&gt; &lt;downloader-pod-name&gt;\n</code></pre>     Look for errors related to accessing storage secrets, connecting to S3/GCS/Git, or filesystem permissions.</p> </li> <li> <p>Check Underlying Resource Events/Logs:</p> <ul> <li>For <code>KaiwoJob</code> -&gt; <code>BatchJob</code>: <code>kubectl describe job -n &lt;namespace&gt; &lt;job-name&gt;</code> and check pod events/logs.</li> <li>For <code>KaiwoJob</code> -&gt; <code>RayJob</code>: <code>kubectl describe rayjob -n &lt;namespace&gt; &lt;job-name&gt;</code> and check Ray cluster/pod events/logs.</li> <li>For <code>KaiwoService</code> -&gt; <code>Deployment</code>: <code>kubectl describe deployment -n &lt;namespace&gt; &lt;service-name&gt;</code> and check pod events/logs.</li> <li>For <code>KaiwoService</code> -&gt; <code>RayService</code>: <code>kubectl describe rayservice -n &lt;namespace&gt; &lt;service-name&gt;</code> and check Ray cluster/pod events/logs.</li> </ul> </li> </ol>"},{"location":"admin/troubleshooting/#pods-not-scheduling-stuck-in-pending","title":"Pods Not Scheduling / Stuck in <code>Pending</code>","text":"<p>This occurs after Kueue admits the workload but before Kubernetes schedules the pod(s).</p> <ol> <li>Describe Pod:     <pre><code>kubectl describe pod -n &lt;namespace&gt; &lt;pod-name&gt;\n</code></pre>     Check the <code>Events</code> section for messages from the scheduler (e.g., <code>FailedScheduling</code>). Common reasons include:<ul> <li>Insufficient Resources: Not enough CPU, memory, or GPUs available on any node.</li> <li>Node Affinity/Selector Mismatch: Pod requires labels that no node possesses (often related to <code>ResourceFlavor</code> <code>nodeLabels</code>).</li> <li>Taint/Toleration Mismatch: Pod lacks tolerations for taints present on suitable nodes (e.g., GPU taint). Kaiwo should add GPU tolerations automatically if GPUs are requested.</li> <li>PVC Binding Issues: If using <code>storage</code>, check if the <code>PersistentVolumeClaim</code> is stuck in <code>Pending</code> (<code>kubectl get pvc -n &lt;namespace&gt;</code>). This could be due to no available <code>PersistentVolume</code> or StorageClass issues.</li> </ul> </li> </ol>"},{"location":"admin/troubleshooting/#pods-crashing-crashloopbackoff","title":"Pods Crashing / <code>CrashLoopBackOff</code>","text":"<ol> <li> <p>Check Pod Logs: This is the most important step.     <pre><code>kubectl logs -n &lt;namespace&gt; &lt;pod-name&gt;\nkubectl logs -n &lt;namespace&gt; &lt;pod-name&gt; -p # Previous container instance logs\n</code></pre>     Look for application errors, missing files, permission issues, OOMKilled errors, GPU driver/runtime errors.</p> </li> <li> <p>Describe Pod: Check events for reasons like OOMKilled.</p> </li> <li> <p>Exec into Pod (if possible): Use <code>kaiwo exec</code> or <code>kubectl exec</code> to inspect the container environment.     <pre><code>kaiwo exec job/&lt;job-name&gt; --command \"/bin/bash\" -n &lt;namespace&gt;\n# or\nkubectl exec -it -n &lt;namespace&gt; &lt;pod-name&gt; -- /bin/bash\n</code></pre></p> </li> </ol>"},{"location":"admin/troubleshooting/#developer-debugging-kaiwo-dev","title":"Developer Debugging (<code>kaiwo-dev</code>)","text":"<p>Info</p> <p>This feature is only intended for contributors</p> <p>The <code>kaiwo-dev</code> tool (built separately from the main CLI/operator) provides debugging utilities.</p> <ul> <li> <p><code>kaiwo-dev debug chainsaw</code>: Helps debug Kyverno Chainsaw E2E tests by collecting and correlating logs and events from a specific test namespace.</p> <p><pre><code># Build the tool: go build -o bin/kaiwo-dev ./pkg/cli/dev/main.go\n./bin/kaiwo-dev debug chainsaw -n &lt;test-namespace&gt; [--print-level &lt;debug|info|warn|error&gt;]\n</code></pre> This command gathers Kaiwo controller logs relevant to the namespace, pod logs within the namespace, and Kubernetes events, sorts them chronologically, and prints them with color-coding. Useful for understanding the sequence of events during a failed test run.</p> </li> </ul>"},{"location":"general/main-components/","title":"Main Components","text":"<p>Kaiwo consists of two main components:</p> <ul> <li>Kaiwo CLI: A command-line interface for submitting and managing workloads to the Kaiwo Operator.</li> <li>Kaiwo Operator: A Kubernetes operator that manages the scheduling and execution of workloads on GPU nodes.   The Kaiwo Operator is responsible for managing the lifecycle of workloads, including scheduling, resource allocation, and monitoring. It leverages the power of Ray and Kueue to provide efficient job queueing and scheduling.</li> </ul>"},{"location":"general/releases/","title":"Releases","text":"<p>Kaiwo releases can be found on the Kaiwo releases page. </p> <p>Each release includes a changelog that lists the new features, bug fixes, and other changes made in that version. In terms of assets, each release includes the following:</p> <ul> <li>install.yaml: The install.yaml file is used to install Kaiwo Operator on your system. For more information on how to install Kaiwo Operator, please refer to the installation guide.</li> <li>kaiwo: The Kaiwo CLI tool, which is the second main component of the Kaiwo project. This is single binary. We release it for all platforms.</li> <li>workloads.zip: The workloads.zip file contains a set of example workloads that can be used to test and demonstrate the capabilities of Kaiwo. These workloads are designed to be easy to use and can be deployed on any Kubernetes cluster. For more information on how to use the workloads, please refer to the quickstart guide for AI Scientists.</li> <li>Source code: The source code for the Kaiwo project is available on GitHub. You can download the source code for any release from the releases page. The source code is licensed under the Apache 2.0 license, which allows you to use, modify, and distribute the code as long as you comply with the terms of the license.</li> </ul>"},{"location":"general/support/","title":"Support","text":"<p>For support please open an issue on Github. We strive to respond to all issues as soon as possible. Please provide as much detail as possible about the issue you are experiencing, including any error messages or screenshots. This will help us to assist you more effectively.</p>"},{"location":"reference/labels/","title":"Label propagation","text":"<p>In order to facilitate logical label propagation, the Kaiwo controller propagates labels from the Kaiwo objects to the target workload objects.</p>"},{"location":"reference/labels/#from-created-kaiwo-resource","title":"From created Kaiwo resource","text":"<p>If you directly create a Kaiwo resource, such as the following job, the labels are taken from two locations, <code>metadata.labels</code>, which are propagated to the downstream resource's metadata field, and <code>spec.podTemplateLabels</code>, which are propagated to the downstream resources' pod template label field(s).</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoJob\nmetadata:\n  name: my-job\n  labels:\n    key1: value1\nspec:\n  user: my-user\n  queue: my-queue\n  podTemplateSpecLabels:\n    key2: value2\n</code></pre> <p>The following batch job would get created</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: my-job\n  labels:\n    key1: value1\n    kueue.x-k8s.io/queue-name: my-queue\n    kaiwo.silogen.ai/type: job\n    kaiwo.silogen.ai/run-id: \"kaiwo-job-UUID\"\n    kaiwo.silogen.ai/user: my-user\n    kaiwo.silogen.ai/name: my-job\nspec:\n  template:\n    metadata:\n      labels:\n        key2: value2\n        kaiwo.silogen.ai/type: job\n        kaiwo.silogen.ai/run-id: \"kaiwo-job-UUID\"\n        kaiwo.silogen.ai/user: my-user\n        kaiwo.silogen.ai/name: my-job\n</code></pre> <p>Which in turn would create the following Pod</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-job-&lt;hash&gt;\n  labels:\n    key2: value2\n    kaiwo.silogen.ai/type: job\n    kaiwo.silogen.ai/run-id: \"kaiwo-job-UUID\"\n    kaiwo.silogen.ai/user: my-user\n    kaiwo.silogen.ai/name: my-job\n</code></pre> <p>Info</p> <p>If you define your job or service spec inline inside the Kaiwo resource definition, these are preserved. However, any label that begins with <code>kaiwo.silogen.ai/</code> may be overwritten, if it clashes with a kaiwo system label such as the ones define above.</p>"},{"location":"reference/labels/#from-directly-created-resource-with-a-kaiwo-label","title":"From directly created resource with a Kaiwo label","text":"<p>If you create a resource such as a batch Job directly and assign the <code>kaiwo.silogen.ai/managed: true</code> label, similar logic is applied.</p> <p>If you create a Job such as </p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: my-job\n  labels:\n    key1: value1\n    kaiwo.silogen.ai/managed: \"true\"\n    kueue.x-k8s.io/queue-name: my-queue\nspec:\n  template:\n    metadata:\n      labels:\n        key2: value2\n</code></pre> <p>This will update the job with the <code>kaiwo.silogen.ai/</code> system labels:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: my-job\n  labels:\n    key1: value1\n    kaiwo.silogen.ai/managed: \"true\"\n    kueue.x-k8s.io/queue-name: my-queue\n    kaiwo.silogen.ai/type: job\n    kaiwo.silogen.ai/run-id: \"kaiwo-job-UUID\"\n    kaiwo.silogen.ai/user: my-user\n    kaiwo.silogen.ai/name: my-job\nspec:\n  template:\n    metadata:\n      labels:\n        key2: value2\n        kaiwo.silogen.ai/type: job\n        kaiwo.silogen.ai/run-id: \"kaiwo-job-UUID\"\n        kaiwo.silogen.ai/user: my-user\n        kaiwo.silogen.ai/name: my-job\n</code></pre> <p>and create a KaiwoJob:</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoJob\nmetadata:\n  name: my-job\n  labels:\n    key1: value1\n    kaiwo.silogen.ai/managed: \"true\"\nspec:\n  queue: my-queue\n</code></pre> <p>Again, if you try to include any protected <code>kaiwo.silogen.ai/</code> labels, these will be overwritten.</p>"},{"location":"reference/crds/config.kaiwo.silogen.ai/","title":"API Reference","text":""},{"location":"reference/crds/config.kaiwo.silogen.ai/#packages","title":"Packages","text":"<ul> <li>config.kaiwo.silogen.ai/v1alpha1</li> </ul>"},{"location":"reference/crds/config.kaiwo.silogen.ai/#configkaiwosilogenaiv1alpha1","title":"config.kaiwo.silogen.ai/v1alpha1","text":"<p>Package v1alpha1 contains API Schema definitions for the kaiwo configuration v1alpha1 API group.</p>"},{"location":"reference/crds/config.kaiwo.silogen.ai/#resource-types","title":"Resource Types","text":"<ul> <li>KaiwoConfig</li> <li>KaiwoConfigList</li> </ul>"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiwoconfig","title":"KaiwoConfig","text":"<p>KaiwoConfig manages the Kaiwo operator's configuration which can be modified during runtime.</p> <p>Appears in: - KaiwoConfigList</p> Field Description Default Validation <code>apiVersion</code> string <code>config.kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoConfig</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> KaiwoConfigSpec Spec defines the desired state for the Kaiwo operator configuration."},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiwoconfiglist","title":"KaiwoConfigList","text":"<p>KaiwoConfigList contains a list of KaiwoConfig resources.</p> Field Description Default Validation <code>apiVersion</code> string <code>config.kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoConfigList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> KaiwoConfig array"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiwoconfigspec","title":"KaiwoConfigSpec","text":"<p>KaiwoConfigSpec defines the desired configuration for the Kaiwo operator's configuration. There should typically be only one KaiwoConfig resource in the cluster.</p> <p>Appears in: - KaiwoConfig</p> Field Description Default Validation <code>ray</code> KaiwoRayConfig Ray defines the Ray-specific settings {  } <code>data</code> KaiwoStorageConfig Storage defines the storage-specific settings {  } <code>nodes</code> KaiwoNodeConfig Nodes defines the node configuration settings {  } <code>scheduling</code> KaiwoSchedulingConfig Scheduling contains the configuration Kaiwo uses for workload scheduling {  } <code>resourceMonitoring</code> KaiwoResourceMonitoringConfig ResourceMonitoring defines the resource-monitoring specific settings {  } <code>defaultClusterQueueName</code> string DefaultClusterQueueName is the name of the default cluster queue that is used for workloads that don't explicitly specify a cluster queue. kaiwo <code>defaultClusterQueueCohortName</code> string DefaultClusterQueueCohortName is the name of the default cohort that is used for the default cluster queue.ClusterQueues in the same cohort can share resources. kaiwo <code>dynamicallyUpdateDefaultClusterQueue</code> boolean DynamicallyUpdateDefaultClusterQueue defines whether the Kaiwo operator should dynamically update default \"kaiwo\" clusterqueue.If set to true, the operator will make sure that the default clusterqueue is always up to date and reflects total resources available.If nodes are added or removed, the operator will update the default clusterqueue to reflect the current state of the cluster. false"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiwonodeconfig","title":"KaiwoNodeConfig","text":"<p>Appears in: - KaiwoConfigSpec</p> Field Description Default Validation <code>defaultGpuResourceKey</code> string DefaultGpuResourceKey defines the default GPU resource key that is used to reserve GPU capacity for pods amd.com/gpu <code>defaultGpuTaintKey</code> string DefaultGpuTaintKey is the key that is used to taint GPU nodes kaiwo.silogen.ai/gpu <code>excludeMasterNodesFromNodePools</code> boolean ExcludeMasterNodesFromNodePools allows excluding the master node(s) from the node pools false <code>addTaintsToGpuNodes</code> boolean AddTaintsToGpuNodes if set to true, will add the DefaultGpuTaintKey taint to the GPU nodes false"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiworayconfig","title":"KaiwoRayConfig","text":"<p>KaiwoRayConfig contains the Ray-specific configuration that Kaiwo uses.</p> <p>Appears in: - KaiwoConfigSpec</p> Field Description Default Validation <code>defaultRayImage</code> string DefaultRayImage is the image that is used for Ray workloads if no image is provided in the workload CRD ghcr.io/silogen/rocm-ray:6.4 <code>headPodMemory</code> string HeadPodMemory is the amount of memory that is requested for the Ray head pod 16Gi"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiworesourcemonitoringconfig","title":"KaiwoResourceMonitoringConfig","text":"<p>KaiwoResourceMonitoringConfig configures the resource monitoring feature. Note that the following must be set as environmental variables inside the Kaiwo controller manager as these cannot be updated without restarting the operator process.</p> <ul> <li>Enabling the resource monitoring feature (<code>RESOURCE_MONITORING_ENABLED=true</code>)</li> <li>Setting the metrics endpoint (<code>RESOURCE_MONITORING_METRICS_ENDPOINT=...</code>)</li> <li>Setting the polling interval (<code>RESOURCE_MONITORING_POLLING_INTERVAL=30s</code>)</li> </ul> <p>Appears in: - KaiwoConfigSpec</p> Field Description Default Validation <code>lowUtilizationThreshold</code> float LowUtilizationThreshold is the threshold which, if the metric goes under, the workload is considered underutilized. The threshold is interpreted as the percentage utilization versus the requested capacity. 1 Minimum: 0  <code>targetNamespaces</code> string array TargetNamespaces is a list of namespaces to apply the monitoring to. If not supplied or empty, all namespaces apart from kube-system will be inspected. However, only pods associated with KaiwoJobs or KaiwoServices are impacted. <code>profile</code> string Profile chooses the target resource to monitor. gpu Enum: [gpu]  <code>terminateUnderutilized</code> boolean TerminateUnderutilized will terminate workloads that are underutilizing resources if set to <code>true</code> false <code>terminateUnderutilizedAfter</code> string TerminateUnderutilizedAfter specifies the duration after which the workload will be terminated if it has been underutilizing resources (for this amount of time) 24h Pattern: <code>^([0-9]+(s\\|m\\|h))+$</code>"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiwoschedulingconfig","title":"KaiwoSchedulingConfig","text":"<p>KaiwoSchedulingConfig contains the configuration Kaiwo uses for workload scheduling</p> <p>Appears in: - KaiwoConfigSpec</p> Field Description Default Validation <code>kubeSchedulerName</code> string KubeSchedulerName defines the default scheduler name that is used to schedule the workload kaiwo-scheduler <code>pendingThresholdForPreemption</code> string PendingThresholdForPreemption is the threshold that is used to determine if a workload is awaiting for compute resources to be available.If the workload is requesting GPUs and pending for longer than this threshold, kaiwo will start preempting workloads that have exceeded their duration deadline and are using GPUs of the same vendor as the pending workload. 5m"},{"location":"reference/crds/config.kaiwo.silogen.ai/#kaiwostorageconfig","title":"KaiwoStorageConfig","text":"<p>Appears in: - KaiwoConfigSpec</p> Field Description Default Validation <code>defaultStorageClass</code> string DefaultStorageClass is the storage class that is used for workloads that don't explicitly specify a storage class. <code>defaultDataMountPath</code> string DefaultDataMountPath is the default path for the data storage and downloads that gets mounted in the workload pods.This value can be overwritten in the workload CRD. /workload <code>defaultHfMountPath</code> string DefaultHfMountPath is the default path for the HuggingFace that gets mounted in the workload pods. The <code>HF_HOME</code> environmental variableis also set to this value. This value can be overwritten in the workload CRD. /hf_cache"},{"location":"reference/crds/kaiwo.silogen.ai/","title":"API Reference","text":""},{"location":"reference/crds/kaiwo.silogen.ai/#packages","title":"Packages","text":"<ul> <li>kaiwo.silogen.ai/v1alpha1</li> </ul>"},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwosilogenaiv1alpha1","title":"kaiwo.silogen.ai/v1alpha1","text":"<p>Package v1alpha1 contains API Schema definitions for the kaiwo v1alpha1 API group.</p>"},{"location":"reference/crds/kaiwo.silogen.ai/#resource-types","title":"Resource Types","text":"<ul> <li>KaiwoJob</li> <li>KaiwoJobList</li> <li>KaiwoQueueConfig</li> <li>KaiwoQueueConfigList</li> <li>KaiwoService</li> <li>KaiwoServiceList</li> </ul>"},{"location":"reference/crds/kaiwo.silogen.ai/#azureblobstoragedownloaditem","title":"AzureBlobStorageDownloadItem","text":"<p>AzureBlobStorageDownloadItem defines parameters for downloading data from Azure Blob Storage.</p> <p>Appears in: - DownloadTaskConfig - ObjectStorageDownloadSpec</p> Field Description Default Validation <code>connectionString</code> ValueReference ConnectionString references a Kubernetes Secret containing the Azure Storage connection string. See <code>ValueReference</code>. <code>containers</code> CloudDownloadBucket array Containers lists the Azure Blob Storage containers and the specific files/folders to download from them. See <code>CloudDownloadBucket</code>."},{"location":"reference/crds/kaiwo.silogen.ai/#clouddownloadbucket","title":"CloudDownloadBucket","text":"<p>CloudDownloadBucket represents a specific bucket (S3, GCS) or container (Azure) to download from.</p> <p>Appears in: - AzureBlobStorageDownloadItem - GCSDownloadItem - S3DownloadItem</p> Field Description Default Validation <code>name</code> string Name is the name of the bucket or container. <code>files</code> CloudDownloadFile array Files lists specific files to download from this bucket/container. <code>folders</code> CloudDownloadFolder array Folders lists specific folders (prefixes) to download from this bucket/container."},{"location":"reference/crds/kaiwo.silogen.ai/#clusterqueue","title":"ClusterQueue","text":"<p>ClusterQueue defines the configuration for a Kueue ClusterQueue managed by Kaiwo.</p> <p>Appears in: - KaiwoQueueConfigSpec</p> Field Description Default Validation <code>name</code> string Name specifies the name of the Kueue ClusterQueue resource. <code>spec</code> ClusterQueueSpec Spec contains the desired Kueue <code>ClusterQueueSpec</code>. Kaiwo ensures the corresponding ClusterQueue resource matches this spec. See Kueue documentation for <code>ClusterQueueSpec</code> fields like <code>resourceGroups</code>, <code>cohort</code>, <code>preemption</code>, etc. <code>namespaces</code> string array Namespaces optionally lists Kubernetes namespaces where Kaiwo should automatically create a Kueue <code>LocalQueue</code> resource pointing to this ClusterQueue.If one or more namespaces are provided, the KaiwoQueueConfig controller takes over managing the LocalQueues for this ClusterQueue.Leave this empty if you want to be able to create your own LocalQueues for this ClusterQueue."},{"location":"reference/crds/kaiwo.silogen.ai/#clusterqueuespec","title":"ClusterQueueSpec","text":"<p>Appears in: - ClusterQueue</p> Field Description Default Validation <code>resourceGroups</code> ResourceGroup array resourceGroups describes groups of resources.Each resource group defines the list of resources and a list of flavorsthat provide quotas for these resources.Each resource and each flavor can only form part of one resource group.resourceGroups can be up to 16. MaxItems: 16  <code>cohort</code> CohortReference cohort that this ClusterQueue belongs to. CQs that belong to thesame cohort can borrow unused resources from each other.A CQ can be a member of a single borrowing cohort. A workload submittedto a queue referencing this CQ can borrow quota from any CQ in the cohort.Only quota for the [resource, flavor] pairs listed in the CQ can beborrowed.If empty, this ClusterQueue cannot borrow from any other ClusterQueue andvice versa.A cohort is a name that links CQs together, but it doesn't reference anyobject. <code>queueingStrategy</code> QueueingStrategy QueueingStrategy indicates the queueing strategy of the workloadsacross the queues in this ClusterQueue.Current Supported Strategies:- StrictFIFO: workloads are ordered strictly by creation time.Older workloads that can't be admitted will block admitting newerworkloads even if they fit available quota.- BestEffortFIFO: workloads are ordered by creation time,however older workloads that can't be admitted will not blockadmitting newer workloads that fit existing quota. BestEffortFIFO Enum: [StrictFIFO BestEffortFIFO]  <code>namespaceSelector</code> LabelSelector namespaceSelector defines which namespaces are allowed to submit workloads tothis clusterQueue. Beyond this basic support for policy, a policy agent likeGatekeeper should be used to enforce more advanced policies.Defaults to null which is a nothing selector (no namespaces eligible).If set to an empty selector <code>\\{\\}</code>, then all namespaces are eligible. <code>flavorFungibility</code> FlavorFungibility flavorFungibility defines whether a workload should try the next flavorbefore borrowing or preempting in the flavor being evaluated. {  } <code>preemption</code> ClusterQueuePreemption {  } <code>admissionChecks</code> AdmissionCheckReference array admissionChecks lists the AdmissionChecks required by this ClusterQueue.Cannot be used along with AdmissionCheckStrategy. <code>admissionChecksStrategy</code> AdmissionChecksStrategy admissionCheckStrategy defines a list of strategies to determine which ResourceFlavors require AdmissionChecks.This property cannot be used in conjunction with the 'admissionChecks' property. <code>stopPolicy</code> StopPolicy stopPolicy - if set to a value different from None, the ClusterQueue is considered Inactive, no new reservation beingmade.Depending on its value, its associated workloads will:- None - Workloads are admitted- HoldAndDrain - Admitted workloads are evicted and Reserving workloads will cancel the reservation.- Hold - Admitted workloads will run to completion and Reserving workloads will cancel the reservation. None Enum: [None Hold HoldAndDrain]  <code>fairSharing</code> FairSharing fairSharing defines the properties of the ClusterQueue whenparticipating in FairSharing.  The values are only relevantif FairSharing is enabled in the Kueue configuration."},{"location":"reference/crds/kaiwo.silogen.ai/#commonmetaspec","title":"CommonMetaSpec","text":"<p>CommonMetaSpec defines reusable metadata fields for workloads.</p> <p>Appears in: - KaiwoJobSpec - KaiwoServiceSpec</p> Field Description Default Validation <code>user</code> string User specifies the owner or creator of the workload. It should typically be the user's email address. This value is primarily used for labeling (<code>kaiwo.silogen.ai/user</code>) the generated resources (like Pods, Jobs, Deployments) for identification and filtering (e.g., with <code>kaiwo list --user &lt;email&gt;</code>).In the future, if authentication is enabled, this must be the email address which is checked against authenticated user for match. <code>podTemplateSpecLabels</code> object (keys:string, values:string) PodTemplateSpecLabels allows you to specify custom labels that will be added to the <code>template.metadata.labels</code> section of the generated Pods (within Jobs, Deployments, or RayCluster specs). Standard Kaiwo system labels (like <code>kaiwo.silogen.ai/user</code>, <code>kaiwo.silogen.ai/name</code>, etc.) are added automatically and take precedence if there are conflicts. <code>gpus</code> integer Gpus specifies the total number of GPUs allocated to the workload. See here for more details on how this field impacts scheduling. 0 <code>gpuVendor</code> string GpuVendor specifies the GPU vendor (e.g., amd, nvidia, etc.). See here for more details on how this field impacts scheduling. amd <code>gpuModels</code> string array GpuModels allows you to optionally specify the GPU models that your workload will run on. You can see available models either by using the CLI and running <code>kaiwo status amd/nvidia</code> or by using kubectl command <code>kubectl get nodes -o custom-columns=NAME:.metadata.name,MODEL:.metadata.labels.kaiwo\\/gpu-model</code>This field is used to filter the available nodes for scheduling. You can specify multiple models, and Kaiwo will select the best available node that matches one of the specified models. <code>version</code> string Version allows you to specify an optional version string for the workload. This can be useful for tracking different iterations or configurations of the same logical workload. It does not directly affect resource creation but serves as metadata. <code>replicas</code> integer Replicas specifies the number of replicas for the workload. See here for more details on how this field impacts scheduling. 1 <code>gpusPerReplica</code> integer GpusPerReplica specifies the number of GPUs allocated per replica. See here for more details on how this field impacts scheduling.If you specify <code>gpusPerReplica</code>, you must also specify <code>replicas</code>. <code>duration</code> Duration Duration specifies the maximum duration over which the workload can run. This is useful for avoiding workloads running indefinitely. <code>preferredTopologyLabel</code> string PreferredTopologyLabel specifies the preferred topology label for scheduling the workload. This is used to influence how the workload is distributed across nodes in the cluster.If not specified, Kaiwo will use the default topology labels defined in the default topology of KaiwoQueueConfig starting at the host level.The levels are evaluated one-by-one going up from the level indicated by the label. If the PodSet cannot fit within a given topology label then the next topology level up is considered.If the PodSet cannot fit at the highest topology level, then it is distributed among multiple topology domains <code>requiredTopologyLabel</code> string RequiredTopologyLabel specifies the required topology label for scheduling the workload. This is used to ensure that the workload is scheduled on nodes that match the specified topology label. <code>resources</code> ResourceRequirements Resources specify the default resource requirements applied for all pods inside the workflow.This field defines default Kubernetes <code>ResourceRequirements</code> (requests and limits for CPU,memory, ephemeral-storage) applied to all containers (including init containers) withinthe workload's pods.Behavior:These values act as defaults. If a container within the underlying Job, Deployment,or Ray spec (if provided by the user) already defines a specific request or limit(e.g., <code>memory</code> limit), the value from <code>resources</code> for that specific metric will not override it.Interaction with GPU fields: The GPU requests/limits (<code>amd.com/gpu</code> or <code>nvidia.com/gpu</code>)are controlled exclusively by the <code>gpus</code>, <code>gpusPerReplica</code>, and <code>gpuVendor</code> fields(and the associated calculation logic described above). Any GPU specifications withinthe <code>resources</code> field are ignored.Default CPU/Memory with GPUs: When Kaiwo generates the underlyingJob/Deployment/RayCluster spec (i.e., the user did not provide <code>spec.job</code>,<code>spec.deployment</code>, or <code>spec.rayService</code>/<code>spec.rayJob</code>), and GPUs are requested(<code>gpusPerReplica</code> &gt; 0), Kaiwo applies default CPU and Memory requests/limitsbased on the GPU count (e.g., 4 CPU cores and 32Gi Memory per GPU).These GPU-derived defaults will override any CPU/Memory settings defined inthe <code>resources</code> field in this specific scenario. If the user does providethe underlying spec, these GPU-derived CPU/Memory defaults are not applied,respecting the user's definition or the values from the <code>resources</code> field. <code>image</code> string Image specifies the default container image to be used for the primary workload container(s).- If containers defined within the underlying Job, Deployment, or Ray spec do not specify an image, this image will be used.- If this field is also empty, the latest tag of ghcr.io/silogen/rocm-ray is used <code>imagePullSecrets</code> LocalObjectReference array ImagePullSecrets is a list of Kubernetes <code>LocalObjectReference</code> (containing just the secret <code>name</code>) referencing secrets needed to pull the container image(s). These are added to the <code>imagePullSecrets</code> field of the PodSpec for all generated pods. <code>env</code> EnvVar array Env is a list of Kubernetes <code>EnvVar</code> structs. These environment variables are added to the primary workload container(s) in the generated pods. They are appended to any environment variables already defined in the underlying Job, Deployment, or Ray spec. <code>secretVolumes</code> SecretVolume array SecretVolumes allows you to mount specific keys from Kubernetes Secrets as files into the workload containers. <code>ray</code> boolean Ray determines whether the operator should use RayCluster for workload execution.If <code>true</code>, Kaiwo will create Ray-specific resources.If <code>false</code> (default), Kaiwo will create standard Kubernetes resources (BatchJob for <code>KaiwoJob</code>, Deployment for <code>KaiwoService</code>).This setting dictates which underlying spec (<code>job</code>/<code>rayJob</code> or <code>deployment</code>/<code>rayService</code>) is primarily used. false <code>storage</code> StorageSpec Storage configures persistent storage using Kubernetes PersistentVolumeClaims (PVCs).Enabling <code>storage.data.download</code> or <code>storage.huggingFace.preCacheRepos</code> will cause Kaiwo to create a temporary Kubernetes Job (the \"download job\") before starting the main workload. This job runs a container that performs the downloads into the respective PVCs. The main workload only starts after the download job completes successfully. <code>dangerous</code> boolean Dangerous, if when set to <code>true</code>, Kaiwo will not add the default <code>PodSecurityContext</code> (which normally sets <code>runAsUser: 1000</code>, <code>runAsGroup: 1000</code>, <code>fsGroup: 1000</code>) to the generated pods. Use this only if you need to run containers as root or a different specific user and understand the security implications. false <code>clusterQueue</code> string ClusterQueue specifies the name of the Kueue <code>ClusterQueue</code> that the workload should be submitted to for scheduling and resource management.This value is set as the <code>kueue.x-k8s.io/queue-name</code> label on the underlying resources.If omitted, it defaults to the value specified by the <code>DEFAULT_CLUSTER_QUEUE_NAME</code> environment variable in the Kaiwo controller (typically \"kaiwo\"), which is set during installation.Note! If the applied KaiwoQueueConfig includes no quota for the default queue, no workload will run that tries to fall back on it.The <code>kaiwo submit</code> CLI command can override this using the <code>--queue</code> flag or the <code>clusterQueue</code> field in the <code>kaiwoconfig.yaml</code> file. <code>priorityClass</code> string WorkloadPriorityClass specifies the name of Kueue <code>WorkloadPriorityClass</code> to be assigned to the job's pods. This influences the scheduling priority relative to other pods in the cluster."},{"location":"reference/crds/kaiwo.silogen.ai/#commonstatusspec","title":"CommonStatusSpec","text":"<p>Appears in: - KaiwoJobStatus - KaiwoServiceStatus</p> Field Description Default Validation <code>startTime</code> Time StartTime records the timestamp when the first pod associated with the workload started running. <code>conditions</code> Condition array Conditions lists the observed conditions of the workload resource, following standard Kubernetes conventions. May include conditions reflecting the underlying Deployment or RayService state. <code>status</code> WorkloadStatus Status reflects the current high-level phase of the workload lifecycle (e.g., PENDING, STARTING, READY, FAILED). <code>duration</code> integer Duration indicates how long the service has been running since StartTime, in seconds. Calculated periodically while running. <code>observedGeneration</code> integer ObservedGeneration records the <code>.metadata.generation</code> of the workload resource that was last processed by the controller."},{"location":"reference/crds/kaiwo.silogen.ai/#datastoragespec","title":"DataStorageSpec","text":"<p>DataStorageSpec configures the primary data volume for the workload.</p> <p>Appears in: - StorageSpec</p> Field Description Default Validation <code>mountPath</code> string MountPath specifies the path inside the workload containers where the data PersistentVolumeClaim will be mounted. /workload <code>storageSize</code> string StorageSize specifies the requested size for the data PersistentVolumeClaim (e.g., \"100Gi\", \"1Ti\"). If set, a PVC will be created. <code>download</code> ObjectStorageDownloadSpec Download configures optional tasks to download data from various sources into the data volume before the main workload starts. See <code>ObjectStorageDownloadSpec</code>."},{"location":"reference/crds/kaiwo.silogen.ai/#gcsdownloaditem","title":"GCSDownloadItem","text":"<p>GCSDownloadItem defines parameters for downloading data from Google Cloud Storage.</p> <p>Appears in: - DownloadTaskConfig - ObjectStorageDownloadSpec</p> Field Description Default Validation <code>applicationCredentials</code> ValueReference ApplicationCredentials references a Kubernetes Secret containing the GCS service account key JSON file content. See <code>ValueReference</code>. <code>buckets</code> CloudDownloadBucket array Buckets lists the GCS buckets and the specific files/folders to download from them. See <code>CloudDownloadBucket</code>."},{"location":"reference/crds/kaiwo.silogen.ai/#gitdownloaditem","title":"GitDownloadItem","text":"<p>GitDownloadItem defines parameters for cloning a Git repository or parts of it.</p> <p>Appears in: - DownloadTaskConfig - ObjectStorageDownloadSpec</p> Field Description Default Validation <code>repository</code> string Repository specifies the Git repository URL (e.g., \"https://github.com/user/repo.git\"). <code>branch</code> string Branch specifies the branch to clone. This takes precedence over <code>commit</code>. <code>commit</code> string Commit specifies the exact commit hash to check out. This is ignored if <code>commit</code> is specified. <code>username</code> ValueReference Username optionally references a Secret containing the Git username for authentication. See <code>ValueReference</code>. <code>token</code> ValueReference Token optionally references a Secret containing the Git token (or password) for authentication. See <code>ValueReference</code>. <code>path</code> string Path specifies a sub-path within the repository to copy. If omitted, the entire repository is copied. <code>targetPath</code> string TargetPath specifies the destination path relative to the data volume's mount point (<code>DataStorageSpec.MountPath</code>) where the repository or <code>path</code> content should be copied."},{"location":"reference/crds/kaiwo.silogen.ai/#hfstoragespec","title":"HfStorageSpec","text":"<p>HfStorageSpec configures storage specifically for Hugging Face model caching.</p> <p>Appears in: - StorageSpec</p> Field Description Default Validation <code>mountPath</code> string MountPath specifies the path inside workload containers where the Hugging Face cache PVC will be mounted.This path is also automatically set as the <code>HF_HOME</code> environment variable in the containers. /hf_cache <code>storageSize</code> string StorageSize specifies the requested size for the Hugging Face cache PersistentVolumeClaim (e.g., \"50Gi\", \"200Gi\"). If set, a PVC will be created. <code>preCacheRepos</code> HuggingFaceDownloadItem array PreCacheRepos is a list of Hugging Face repositories to download into the cache volume before the main workload starts."},{"location":"reference/crds/kaiwo.silogen.ai/#huggingfacedownloaditem","title":"HuggingFaceDownloadItem","text":"<p>HuggingFaceDownloadItem defines parameters for pre-caching a Hugging Face repository or specific files from it.</p> <p>Appears in: - DownloadTaskConfig - HfStorageSpec</p> Field Description Default Validation <code>repoId</code> string RepoID is the Hugging Face Hub repository ID (e.g., \"meta-llama/Llama-2-7b-chat-hf\"). <code>files</code> string array Files is an optional list of specific files to download from the repository. If omitted, the entire repository is downloaded."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwojob","title":"KaiwoJob","text":"<p>KaiwoJob represents a batch workload managed by Kaiwo. It encapsulates either a standard Kubernetes Job or a RayJob, along with common metadata, storage configurations, and scheduling preferences. The Kaiwo controller reconciles this resource to create and manage the underlying workload objects.</p> <p>Appears in: - KaiwoJobList</p> Field Description Default Validation <code>apiVersion</code> string <code>kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoJob</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> KaiwoJobSpec Spec defines the desired state of the KaiwoJob, including workload type (Job/RayJob), configuration, resources, and common metadata. <code>status</code> KaiwoJobStatus Status reflects the most recently observed state of the KaiwoJob, including its phase, start/completion times, and conditions."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwojoblist","title":"KaiwoJobList","text":"<p>KaiwoJobList</p> Field Description Default Validation <code>apiVersion</code> string <code>kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoJobList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> KaiwoJob array"},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwojobspec","title":"KaiwoJobSpec","text":"<p>KaiwoJobSpec defines the desired state of KaiwoJob.</p> <p>Appears in: - KaiwoJob</p> Field Description Default Validation <code>user</code> string User specifies the owner or creator of the workload. It should typically be the user's email address. This value is primarily used for labeling (<code>kaiwo.silogen.ai/user</code>) the generated resources (like Pods, Jobs, Deployments) for identification and filtering (e.g., with <code>kaiwo list --user &lt;email&gt;</code>).In the future, if authentication is enabled, this must be the email address which is checked against authenticated user for match. <code>podTemplateSpecLabels</code> object (keys:string, values:string) PodTemplateSpecLabels allows you to specify custom labels that will be added to the <code>template.metadata.labels</code> section of the generated Pods (within Jobs, Deployments, or RayCluster specs). Standard Kaiwo system labels (like <code>kaiwo.silogen.ai/user</code>, <code>kaiwo.silogen.ai/name</code>, etc.) are added automatically and take precedence if there are conflicts. <code>gpus</code> integer Gpus specifies the total number of GPUs allocated to the workload. See here for more details on how this field impacts scheduling. 0 <code>gpuVendor</code> string GpuVendor specifies the GPU vendor (e.g., amd, nvidia, etc.). See here for more details on how this field impacts scheduling. amd <code>gpuModels</code> string array GpuModels allows you to optionally specify the GPU models that your workload will run on. You can see available models either by using the CLI and running <code>kaiwo status amd/nvidia</code> or by using kubectl command <code>kubectl get nodes -o custom-columns=NAME:.metadata.name,MODEL:.metadata.labels.kaiwo\\/gpu-model</code>This field is used to filter the available nodes for scheduling. You can specify multiple models, and Kaiwo will select the best available node that matches one of the specified models. <code>version</code> string Version allows you to specify an optional version string for the workload. This can be useful for tracking different iterations or configurations of the same logical workload. It does not directly affect resource creation but serves as metadata. <code>replicas</code> integer Replicas specifies the number of replicas for the workload. See here for more details on how this field impacts scheduling. 1 <code>gpusPerReplica</code> integer GpusPerReplica specifies the number of GPUs allocated per replica. See here for more details on how this field impacts scheduling.If you specify <code>gpusPerReplica</code>, you must also specify <code>replicas</code>. <code>duration</code> Duration Duration specifies the maximum duration over which the workload can run. This is useful for avoiding workloads running indefinitely. <code>preferredTopologyLabel</code> string PreferredTopologyLabel specifies the preferred topology label for scheduling the workload. This is used to influence how the workload is distributed across nodes in the cluster.If not specified, Kaiwo will use the default topology labels defined in the default topology of KaiwoQueueConfig starting at the host level.The levels are evaluated one-by-one going up from the level indicated by the label. If the PodSet cannot fit within a given topology label then the next topology level up is considered.If the PodSet cannot fit at the highest topology level, then it is distributed among multiple topology domains <code>requiredTopologyLabel</code> string RequiredTopologyLabel specifies the required topology label for scheduling the workload. This is used to ensure that the workload is scheduled on nodes that match the specified topology label. <code>resources</code> ResourceRequirements Resources specify the default resource requirements applied for all pods inside the workflow.This field defines default Kubernetes <code>ResourceRequirements</code> (requests and limits for CPU,memory, ephemeral-storage) applied to all containers (including init containers) withinthe workload's pods.Behavior:These values act as defaults. If a container within the underlying Job, Deployment,or Ray spec (if provided by the user) already defines a specific request or limit(e.g., <code>memory</code> limit), the value from <code>resources</code> for that specific metric will not override it.Interaction with GPU fields: The GPU requests/limits (<code>amd.com/gpu</code> or <code>nvidia.com/gpu</code>)are controlled exclusively by the <code>gpus</code>, <code>gpusPerReplica</code>, and <code>gpuVendor</code> fields(and the associated calculation logic described above). Any GPU specifications withinthe <code>resources</code> field are ignored.Default CPU/Memory with GPUs: When Kaiwo generates the underlyingJob/Deployment/RayCluster spec (i.e., the user did not provide <code>spec.job</code>,<code>spec.deployment</code>, or <code>spec.rayService</code>/<code>spec.rayJob</code>), and GPUs are requested(<code>gpusPerReplica</code> &gt; 0), Kaiwo applies default CPU and Memory requests/limitsbased on the GPU count (e.g., 4 CPU cores and 32Gi Memory per GPU).These GPU-derived defaults will override any CPU/Memory settings defined inthe <code>resources</code> field in this specific scenario. If the user does providethe underlying spec, these GPU-derived CPU/Memory defaults are not applied,respecting the user's definition or the values from the <code>resources</code> field. <code>image</code> string Image specifies the default container image to be used for the primary workload container(s).- If containers defined within the underlying Job, Deployment, or Ray spec do not specify an image, this image will be used.- If this field is also empty, the latest tag of ghcr.io/silogen/rocm-ray is used <code>imagePullSecrets</code> LocalObjectReference array ImagePullSecrets is a list of Kubernetes <code>LocalObjectReference</code> (containing just the secret <code>name</code>) referencing secrets needed to pull the container image(s). These are added to the <code>imagePullSecrets</code> field of the PodSpec for all generated pods. <code>env</code> EnvVar array Env is a list of Kubernetes <code>EnvVar</code> structs. These environment variables are added to the primary workload container(s) in the generated pods. They are appended to any environment variables already defined in the underlying Job, Deployment, or Ray spec. <code>secretVolumes</code> SecretVolume array SecretVolumes allows you to mount specific keys from Kubernetes Secrets as files into the workload containers. <code>ray</code> boolean Ray determines whether the operator should use RayCluster for workload execution.If <code>true</code>, Kaiwo will create Ray-specific resources.If <code>false</code> (default), Kaiwo will create standard Kubernetes resources (BatchJob for <code>KaiwoJob</code>, Deployment for <code>KaiwoService</code>).This setting dictates which underlying spec (<code>job</code>/<code>rayJob</code> or <code>deployment</code>/<code>rayService</code>) is primarily used. false <code>storage</code> StorageSpec Storage configures persistent storage using Kubernetes PersistentVolumeClaims (PVCs).Enabling <code>storage.data.download</code> or <code>storage.huggingFace.preCacheRepos</code> will cause Kaiwo to create a temporary Kubernetes Job (the \"download job\") before starting the main workload. This job runs a container that performs the downloads into the respective PVCs. The main workload only starts after the download job completes successfully. <code>dangerous</code> boolean Dangerous, if when set to <code>true</code>, Kaiwo will not add the default <code>PodSecurityContext</code> (which normally sets <code>runAsUser: 1000</code>, <code>runAsGroup: 1000</code>, <code>fsGroup: 1000</code>) to the generated pods. Use this only if you need to run containers as root or a different specific user and understand the security implications. false <code>clusterQueue</code> string ClusterQueue specifies the name of the Kueue <code>ClusterQueue</code> that the workload should be submitted to for scheduling and resource management.This value is set as the <code>kueue.x-k8s.io/queue-name</code> label on the underlying resources.If omitted, it defaults to the value specified by the <code>DEFAULT_CLUSTER_QUEUE_NAME</code> environment variable in the Kaiwo controller (typically \"kaiwo\"), which is set during installation.Note! If the applied KaiwoQueueConfig includes no quota for the default queue, no workload will run that tries to fall back on it.The <code>kaiwo submit</code> CLI command can override this using the <code>--queue</code> flag or the <code>clusterQueue</code> field in the <code>kaiwoconfig.yaml</code> file. <code>priorityClass</code> string WorkloadPriorityClass specifies the name of Kueue <code>WorkloadPriorityClass</code> to be assigned to the job's pods. This influences the scheduling priority relative to other pods in the cluster. <code>entrypoint</code> string EntryPoint defines the command or script that the primary container in the job's pod(s) should execute.It can be a multi-line string. Shell script shebangs (<code>#!/bin/bash</code>) are detected.For standard Kubernetes Jobs (<code>ray: false</code>), this populates the <code>command</code> and <code>args</code> fields of the container spec (typically <code>[\"/bin/sh\", \"-c\", \"&lt;entrypoint_script&gt;\"]</code>).For RayJobs (<code>ray: true</code>), this populates the <code>rayJob.spec.entrypoint</code> field. For RayJobs, this must reference a Python script.This overrides any default command specified in the container image or the underlying <code>job</code> or <code>rayJob</code> spec sections if they are also defined. <code>rayJob</code> RayJob RayJob defines the RayJob configuration.If this field is present (or if <code>spec.ray</code> is <code>true</code>), Kaiwo will create a <code>RayJob</code> resource instead of a standard <code>batchv1.Job</code>.Common fields like <code>image</code>, <code>resources</code>, <code>gpus</code>, <code>replicas</code>, etc., will be merged into this spec, potentially overriding values defined here unless explicitly configured otherwise.This provides fine-grained control over the Ray cluster configuration (head/worker groups) and Ray job submission parameters. <code>job</code> Job Job defines the Kubernetes Job configuration.If this field is present and <code>spec.ray</code> is <code>false</code>, Kaiwo will use this as the base for the created <code>batchv1.Job</code>.Common fields like <code>image</code>, <code>resources</code>, <code>gpus</code>, <code>entrypoint</code>, etc., will be merged into this spec, potentially overriding values defined here.This provides fine-grained control over standard Kubernetes Job parameters like <code>backoffLimit</code>, <code>ttlSecondsAfterFinished</code>, pod template details, etc."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwojobstatus","title":"KaiwoJobStatus","text":"<p>KaiwoJobStatus defines the observed state of KaiwoJob.</p> <p>Appears in: - KaiwoJob</p> Field Description Default Validation <code>startTime</code> Time StartTime records the timestamp when the first pod associated with the workload started running. <code>conditions</code> Condition array Conditions lists the observed conditions of the workload resource, following standard Kubernetes conventions. May include conditions reflecting the underlying Deployment or RayService state. <code>status</code> WorkloadStatus Status reflects the current high-level phase of the workload lifecycle (e.g., PENDING, STARTING, READY, FAILED). <code>duration</code> integer Duration indicates how long the service has been running since StartTime, in seconds. Calculated periodically while running. <code>observedGeneration</code> integer ObservedGeneration records the <code>.metadata.generation</code> of the workload resource that was last processed by the controller. <code>completionTime</code> Time CompletionTime records the timestamp when the KaiwoJob finished execution (either successfully or with failure)."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoqueueconfig","title":"KaiwoQueueConfig","text":"<p>KaiwoQueueConfig manages Kueue resources like ClusterQueues, ResourceFlavors, and WorkloadPriorityClasses based on its spec. It acts as a central configuration point for Kaiwo's integration with Kueue. Typically, only one cluster-scoped resource named 'kaiwo' should exist. The controller ensures that the specified Kueue resources are created, updated, or deleted to match the desired state defined here. KaiwoQueueConfig manages Kueue resources.</p> <p>Appears in: - KaiwoQueueConfigList</p> Field Description Default Validation <code>apiVersion</code> string <code>kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoQueueConfig</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> KaiwoQueueConfigSpec Spec defines the desired state for Kueue resources managed by Kaiwo. <code>status</code> KaiwoQueueConfigStatus Status reflects the most recently observed state of the Kueue resource synchronization."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoqueueconfiglist","title":"KaiwoQueueConfigList","text":"<p>KaiwoQueueConfigList contains a list of KaiwoQueueConfig resources.</p> Field Description Default Validation <code>apiVersion</code> string <code>kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoQueueConfigList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> KaiwoQueueConfig array"},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoqueueconfigspec","title":"KaiwoQueueConfigSpec","text":"<p>KaiwoQueueConfigSpec defines the desired configuration for Kaiwo's management of Kueue resources. There should typically be only one KaiwoQueueConfig resource in the cluster, named 'kaiwo'.</p> <p>Appears in: - KaiwoQueueConfig</p> Field Description Default Validation <code>clusterQueues</code> ClusterQueue array ClusterQueues defines a list of Kueue ClusterQueues that Kaiwo should manage. Kaiwo ensures these ClusterQueues exist and match the provided specs. MaxItems: 1000  <code>resourceFlavors</code> ResourceFlavorSpec array ResourceFlavors defines a list of Kueue ResourceFlavors that Kaiwo should manage. Kaiwo ensures these ResourceFlavors exist and match the provided specs. If omitted or empty, Kaiwo attempts to automatically discover node pools and create default flavors based on node labels. MaxItems: 20  <code>workloadPriorityClasses</code> WorkloadPriorityClass array WorkloadPriorityClasses defines a list of Kueue WorkloadPriorityClasses that Kaiwo should manage. Kaiwo ensures these priority classes exist with the specified values. See Kueue documentation for <code>WorkloadPriorityClass</code>. MaxItems: 20  <code>topologies</code> Topology array Topologies defines a list of Kueue Topologies that Kaiwo should manage. Kaiwo ensures these Topologies exist with the specified values. See Kueue documentation for <code>Topology</code>. MaxItems: 10"},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoqueueconfigstatus","title":"KaiwoQueueConfigStatus","text":"<p>KaiwoQueueConfigStatus represents the observed state of KaiwoQueueConfig.</p> <p>Appears in: - KaiwoQueueConfig</p> Field Description Default Validation <code>conditions</code> Condition array Conditions lists the observed conditions of the KaiwoQueueConfig resource, such as whether the managed Kueue resources are synchronized and ready. <code>status</code> QueueConfigStatusDescription Status reflects the overall status of the Kueue resource synchronization managed by this config (e.g., READY, FAILED)."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoservice","title":"KaiwoService","text":"<p>KaiwoService represents a long-running service workload managed by Kaiwo. It encapsulates either a standard Kubernetes Deployment  or a RayService (via an AppWrapper), along with common metadata, storage configurations, and scheduling preferences. The Kaiwo controller reconciles this resource to create and manage the underlying workload objects.</p> <p>Appears in: - KaiwoServiceList</p> Field Description Default Validation <code>apiVersion</code> string <code>kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoService</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> KaiwoServiceSpec Spec defines the desired state of the KaiwoService, including workload type (Deployment/RayService), configuration, resources, and common metadata. <code>status</code> KaiwoServiceStatus Status reflects the most recently observed state of the KaiwoService, including its phase, start time, duration, and conditions."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoservicelist","title":"KaiwoServiceList","text":"<p>KaiwoServiceList</p> Field Description Default Validation <code>apiVersion</code> string <code>kaiwo.silogen.ai/v1alpha1</code> <code>kind</code> string <code>KaiwoServiceList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> KaiwoService array"},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoservicespec","title":"KaiwoServiceSpec","text":"<p>KaiwoServiceSpec defines the desired state of KaiwoService.</p> <p>Appears in: - KaiwoService</p> Field Description Default Validation <code>user</code> string User specifies the owner or creator of the workload. It should typically be the user's email address. This value is primarily used for labeling (<code>kaiwo.silogen.ai/user</code>) the generated resources (like Pods, Jobs, Deployments) for identification and filtering (e.g., with <code>kaiwo list --user &lt;email&gt;</code>).In the future, if authentication is enabled, this must be the email address which is checked against authenticated user for match. <code>podTemplateSpecLabels</code> object (keys:string, values:string) PodTemplateSpecLabels allows you to specify custom labels that will be added to the <code>template.metadata.labels</code> section of the generated Pods (within Jobs, Deployments, or RayCluster specs). Standard Kaiwo system labels (like <code>kaiwo.silogen.ai/user</code>, <code>kaiwo.silogen.ai/name</code>, etc.) are added automatically and take precedence if there are conflicts. <code>gpus</code> integer Gpus specifies the total number of GPUs allocated to the workload. See here for more details on how this field impacts scheduling. 0 <code>gpuVendor</code> string GpuVendor specifies the GPU vendor (e.g., amd, nvidia, etc.). See here for more details on how this field impacts scheduling. amd <code>gpuModels</code> string array GpuModels allows you to optionally specify the GPU models that your workload will run on. You can see available models either by using the CLI and running <code>kaiwo status amd/nvidia</code> or by using kubectl command <code>kubectl get nodes -o custom-columns=NAME:.metadata.name,MODEL:.metadata.labels.kaiwo\\/gpu-model</code>This field is used to filter the available nodes for scheduling. You can specify multiple models, and Kaiwo will select the best available node that matches one of the specified models. <code>version</code> string Version allows you to specify an optional version string for the workload. This can be useful for tracking different iterations or configurations of the same logical workload. It does not directly affect resource creation but serves as metadata. <code>replicas</code> integer Replicas specifies the number of replicas for the workload. See here for more details on how this field impacts scheduling. 1 <code>gpusPerReplica</code> integer GpusPerReplica specifies the number of GPUs allocated per replica. See here for more details on how this field impacts scheduling.If you specify <code>gpusPerReplica</code>, you must also specify <code>replicas</code>. <code>duration</code> Duration Duration specifies the maximum duration over which the workload can run. This is useful for avoiding workloads running indefinitely. <code>preferredTopologyLabel</code> string PreferredTopologyLabel specifies the preferred topology label for scheduling the workload. This is used to influence how the workload is distributed across nodes in the cluster.If not specified, Kaiwo will use the default topology labels defined in the default topology of KaiwoQueueConfig starting at the host level.The levels are evaluated one-by-one going up from the level indicated by the label. If the PodSet cannot fit within a given topology label then the next topology level up is considered.If the PodSet cannot fit at the highest topology level, then it is distributed among multiple topology domains <code>requiredTopologyLabel</code> string RequiredTopologyLabel specifies the required topology label for scheduling the workload. This is used to ensure that the workload is scheduled on nodes that match the specified topology label. <code>resources</code> ResourceRequirements Resources specify the default resource requirements applied for all pods inside the workflow.This field defines default Kubernetes <code>ResourceRequirements</code> (requests and limits for CPU,memory, ephemeral-storage) applied to all containers (including init containers) withinthe workload's pods.Behavior:These values act as defaults. If a container within the underlying Job, Deployment,or Ray spec (if provided by the user) already defines a specific request or limit(e.g., <code>memory</code> limit), the value from <code>resources</code> for that specific metric will not override it.Interaction with GPU fields: The GPU requests/limits (<code>amd.com/gpu</code> or <code>nvidia.com/gpu</code>)are controlled exclusively by the <code>gpus</code>, <code>gpusPerReplica</code>, and <code>gpuVendor</code> fields(and the associated calculation logic described above). Any GPU specifications withinthe <code>resources</code> field are ignored.Default CPU/Memory with GPUs: When Kaiwo generates the underlyingJob/Deployment/RayCluster spec (i.e., the user did not provide <code>spec.job</code>,<code>spec.deployment</code>, or <code>spec.rayService</code>/<code>spec.rayJob</code>), and GPUs are requested(<code>gpusPerReplica</code> &gt; 0), Kaiwo applies default CPU and Memory requests/limitsbased on the GPU count (e.g., 4 CPU cores and 32Gi Memory per GPU).These GPU-derived defaults will override any CPU/Memory settings defined inthe <code>resources</code> field in this specific scenario. If the user does providethe underlying spec, these GPU-derived CPU/Memory defaults are not applied,respecting the user's definition or the values from the <code>resources</code> field. <code>image</code> string Image specifies the default container image to be used for the primary workload container(s).- If containers defined within the underlying Job, Deployment, or Ray spec do not specify an image, this image will be used.- If this field is also empty, the latest tag of ghcr.io/silogen/rocm-ray is used <code>imagePullSecrets</code> LocalObjectReference array ImagePullSecrets is a list of Kubernetes <code>LocalObjectReference</code> (containing just the secret <code>name</code>) referencing secrets needed to pull the container image(s). These are added to the <code>imagePullSecrets</code> field of the PodSpec for all generated pods. <code>env</code> EnvVar array Env is a list of Kubernetes <code>EnvVar</code> structs. These environment variables are added to the primary workload container(s) in the generated pods. They are appended to any environment variables already defined in the underlying Job, Deployment, or Ray spec. <code>secretVolumes</code> SecretVolume array SecretVolumes allows you to mount specific keys from Kubernetes Secrets as files into the workload containers. <code>ray</code> boolean Ray determines whether the operator should use RayCluster for workload execution.If <code>true</code>, Kaiwo will create Ray-specific resources.If <code>false</code> (default), Kaiwo will create standard Kubernetes resources (BatchJob for <code>KaiwoJob</code>, Deployment for <code>KaiwoService</code>).This setting dictates which underlying spec (<code>job</code>/<code>rayJob</code> or <code>deployment</code>/<code>rayService</code>) is primarily used. false <code>storage</code> StorageSpec Storage configures persistent storage using Kubernetes PersistentVolumeClaims (PVCs).Enabling <code>storage.data.download</code> or <code>storage.huggingFace.preCacheRepos</code> will cause Kaiwo to create a temporary Kubernetes Job (the \"download job\") before starting the main workload. This job runs a container that performs the downloads into the respective PVCs. The main workload only starts after the download job completes successfully. <code>dangerous</code> boolean Dangerous, if when set to <code>true</code>, Kaiwo will not add the default <code>PodSecurityContext</code> (which normally sets <code>runAsUser: 1000</code>, <code>runAsGroup: 1000</code>, <code>fsGroup: 1000</code>) to the generated pods. Use this only if you need to run containers as root or a different specific user and understand the security implications. false <code>clusterQueue</code> string ClusterQueue specifies the name of the Kueue <code>ClusterQueue</code> that the workload should be submitted to for scheduling and resource management.This value is set as the <code>kueue.x-k8s.io/queue-name</code> label on the underlying resources.If omitted, it defaults to the value specified by the <code>DEFAULT_CLUSTER_QUEUE_NAME</code> environment variable in the Kaiwo controller (typically \"kaiwo\"), which is set during installation.Note! If the applied KaiwoQueueConfig includes no quota for the default queue, no workload will run that tries to fall back on it.The <code>kaiwo submit</code> CLI command can override this using the <code>--queue</code> flag or the <code>clusterQueue</code> field in the <code>kaiwoconfig.yaml</code> file. <code>priorityClass</code> string WorkloadPriorityClass specifies the name of Kueue <code>WorkloadPriorityClass</code> to be assigned to the job's pods. This influences the scheduling priority relative to other pods in the cluster. <code>entrypoint</code> string EntryPoint specifies the command or script executed in a Deployment.Can also be defined inside Deployment struct as regular command in the form of string array.It is not used when <code>ray: true</code> (use <code>serveConfigV2</code> or the <code>rayService</code> spec instead for Ray entrypoints). <code>serveConfigV2</code> string Defines the applications and deployments to deploy, should be a YAML multi-line scalar string.Can also be defined inside RayService struct <code>rayService</code> RayService RayService allows providing a full <code>rayv1.RayService</code> spec.If present (or <code>spec.ray</code> is <code>true</code>), Kaiwo creates a <code>RayService</code> (wrapped in an AppWrapper for Kueue integration) instead of a <code>Deployment</code>.Common fields are merged into the <code>RayClusterSpec</code> within this spec.Allows fine-grained control over the Ray cluster and Ray Serve configurations. <code>deployment</code> Deployment Deployment allows providing a full <code>appsv1.Deployment</code> spec.If present and <code>spec.ray</code> is <code>false</code>, this is used as the base for the created <code>Deployment</code>.Common fields are merged into this spec.Allows fine-grained control over Kubernetes Deployment parameters (strategy, selectors, pod template, etc.)."},{"location":"reference/crds/kaiwo.silogen.ai/#kaiwoservicestatus","title":"KaiwoServiceStatus","text":"<p>KaiwoServiceStatus defines the observed state of KaiwoService.</p> <p>Appears in: - KaiwoService</p> Field Description Default Validation <code>startTime</code> Time StartTime records the timestamp when the first pod associated with the workload started running. <code>conditions</code> Condition array Conditions lists the observed conditions of the workload resource, following standard Kubernetes conventions. May include conditions reflecting the underlying Deployment or RayService state. <code>status</code> WorkloadStatus Status reflects the current high-level phase of the workload lifecycle (e.g., PENDING, STARTING, READY, FAILED). <code>duration</code> integer Duration indicates how long the service has been running since StartTime, in seconds. Calculated periodically while running. <code>observedGeneration</code> integer ObservedGeneration records the <code>.metadata.generation</code> of the workload resource that was last processed by the controller."},{"location":"reference/crds/kaiwo.silogen.ai/#objectstoragedownloadspec","title":"ObjectStorageDownloadSpec","text":"<p>ObjectStorageDownloadSpec aggregates download tasks for various object storage and Git sources within the <code>DataStorageSpec</code>.</p> <p>Appears in: - DataStorageSpec</p> Field Description Default Validation <code>s3</code> S3DownloadItem array S3 lists any S3 downloads <code>gcs</code> GCSDownloadItem array GCS lists and Google Cloud Storage downloads <code>azureBlob</code> AzureBlobStorageDownloadItem array AzureBlob lists any Azure Blob Storage downloads <code>git</code> GitDownloadItem array Git lists any Git downloads"},{"location":"reference/crds/kaiwo.silogen.ai/#queueconfigstatusdescription","title":"QueueConfigStatusDescription","text":"<p>Underlying type: string</p> <p>Appears in: - KaiwoQueueConfigStatus</p> Field Description <code>READY</code> <code>FAILED</code>"},{"location":"reference/crds/kaiwo.silogen.ai/#resourceflavorspec","title":"ResourceFlavorSpec","text":"<p>ResourceFlavorSpec defines the configuration for a Kueue ResourceFlavor managed by Kaiwo.</p> <p>Appears in: - KaiwoQueueConfigSpec</p> Field Description Default Validation <code>name</code> string Name specifies the name of the Kueue ResourceFlavor resource (e.g., \"amd-mi300-8gpu\"). <code>nodeLabels</code> object (keys:string, values:string) NodeLabels specifies the labels that pods requesting this flavor must match on nodes. This is used by Kueue for scheduling decisions. Keys and values should correspond to actual node labels. Example: <code>\\{\"kaiwo/nodepool\": \"amd-gpu-nodes\"\\}</code> MaxProperties: 10  <code>taints</code> Taint array Taints specifies a list of taints associated with this flavor. MaxItems: 5  <code>tolerations</code> Toleration array Tolerations specifies a list of tolerations associated with this flavor. This is less common than using Taints; Kueue primarily uses Taints to derive Tolerations. MaxItems: 5  <code>topologyName</code> string TopologyName specifies the name of the Kueue Topology that this flavor belongs to. If specified, it must match one of the Topologies defined in the KaiwoQueueConfig.This is used to group flavors by topology for scheduling purposes."},{"location":"reference/crds/kaiwo.silogen.ai/#s3downloaditem","title":"S3DownloadItem","text":"<p>S3DownloadItem defines parameters for downloading data from an S3-compatible object store.</p> <p>Appears in: - DownloadTaskConfig - ObjectStorageDownloadSpec</p> Field Description Default Validation <code>endpointUrl</code> string EndpointUrl specifies the S3 API endpoint URL (e.g., \"https://s3.us-east-1.amazonaws.com\" or a MinIO endpoint). <code>accessKeyId</code> ValueReference AccessKeyId optionally references a Kubernetes Secret containing the S3 access key ID. See <code>ValueReference</code>. <code>secretKey</code> ValueReference SecretKey optionally references a Kubernetes Secret containing the S3 secret access key. See <code>ValueReference</code>. <code>buckets</code> CloudDownloadBucket array Buckets lists the S3 buckets and the specific files/folders to download from them. See <code>CloudDownloadBucket</code>."},{"location":"reference/crds/kaiwo.silogen.ai/#secretvolume","title":"SecretVolume","text":"<p>SecretVolume defines how to mount a specific key from a Kubernetes Secret into the workload's containers.</p> <p>Appears in: - CommonMetaSpec - KaiwoJobSpec - KaiwoServiceSpec</p> Field Description Default Validation <code>name</code> string Name defines the name of the Kubernetes Volume that will be created. Should be unique within the pod. <code>secretName</code> string SecretName specifies the name of the Kubernetes Secret resource to mount from. <code>key</code> string Key specifies the key within the Secret whose value should be mounted. If omitted, the entire secret might be mounted as files (depending on Kubernetes behavior). <code>subPath</code> string SubPath defines the filename within the <code>MountPath</code> directory where the secret <code>Key</code>'s content will be placed. Useful for mounting a single secret key as a file. <code>mountPath</code> string MountPath defines the directory path inside the container where the secret volume (or the <code>SubPath</code> file) should be mounted."},{"location":"reference/crds/kaiwo.silogen.ai/#storagespec","title":"StorageSpec","text":"<p>StorageSpec defines the storage configuration for the workload.</p> <p>Appears in: - CommonMetaSpec - KaiwoJobSpec - KaiwoServiceSpec</p> Field Description Default Validation <code>storageEnabled</code> boolean StorageEnabled must be <code>true</code> to enable the creation of any PersistentVolumeClaims defined within this spec. If <code>false</code>, <code>data</code> and <code>huggingFace</code> sections are ignored. <code>storageClassName</code> string StorageClassName specifies the name of the Kubernetes <code>StorageClass</code> to use when creating PersistentVolumeClaims for <code>data</code> and <code>huggingFace</code> volumes. Must refer to an existing StorageClass in the cluster. <code>accessMode</code> PersistentVolumeAccessMode AccessMode determines the access mode (e.g., <code>ReadWriteOnce</code>, <code>ReadWriteMany</code>, <code>ReadOnlyMany</code>) for the created PersistentVolumeClaims.In a multi-node setting, ReadWriteMany is generally required, as pods scheduled on different nodes cannot access ReadWriteOnce PVCs. This is true even when <code>replicas: 1</code> if you are using download jobs, as the download pod may get scheduled on a different pod than the main workload pod. ReadWriteMany <code>data</code> DataStorageSpec Data configures the main data PersistentVolumeClaim and optional pre-download tasks for it. <code>huggingFace</code> HfStorageSpec HuggingFace configures a PersistentVolumeClaim specifically for caching Hugging Face models and datasets, with options for pre-caching."},{"location":"reference/crds/kaiwo.silogen.ai/#topology","title":"Topology","text":"<p>Topology is the Schema for the topology API</p> <p>Appears in: - KaiwoQueueConfigSpec</p> Field Description Default Validation <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> TopologySpec Required: {}"},{"location":"reference/crds/kaiwo.silogen.ai/#topologyspec","title":"TopologySpec","text":"<p>Appears in: - Topology</p> Field Description Default Validation <code>levels</code> TopologyLevel array levels define the levels of topology. MaxItems: 8 MinItems: 1"},{"location":"reference/crds/kaiwo.silogen.ai/#valuereference","title":"ValueReference","text":"<p>ValueReference provides a way to reference sensitive values stored in Kubernetes Secrets, typically used for credentials needed by download tasks.</p> <p>Appears in: - AzureBlobStorageDownloadItem - GCSDownloadItem - GitDownloadItem - S3DownloadItem</p> Field Description Default Validation <code>file</code> string File specifies the expected path within the download job's container where the secret value will be mounted as a file. This path is usually automatically generated by the controller based on SecretName and SecretKey. <code>secretName</code> string SecretName is the name of the Kubernetes Secret resource containing the value. <code>secretKey</code> string SecretKey is the key within the specified Secret whose value should be used."},{"location":"reference/crds/kaiwo.silogen.ai/#workloadstatus","title":"WorkloadStatus","text":"<p>Underlying type: string</p> <p>Appears in: - CommonStatusSpec - KaiwoJobStatus - KaiwoServiceStatus</p> Field Description `` WorkloadStatusNew indicates the resource has been created but not yet processed by the controller. <code>DOWNLOADING</code> WorkloadStatusDownloading indicates that the resource is currently running the download job <code>PENDING</code> WorkloadStatusPending indicates the resource is waiting for prerequisites (like Kueue admission) to complete. <code>STARTING</code> WorkloadStatusStarting indicates the Kaiwo workload has been admitted, and the underlying workload (Job, Deployment, RayService) is being created or started. <code>RUNNING</code> WorkloadStatusRunning indicates the workload pods are running. For KaiwoJob, this means the job has started execution. For KaiwoService, pods are up but may not yet be fully ready/healthy. <code>COMPLETE</code> WorkloadStatusComplete indicates a KaiwoJob has finished successfully. <code>ERROR</code> WorkloadStatusError indicates the workload encountered an error which can be recovered from. <code>FAILED</code> WorkloadStatusFailed indicates the workload (KaiwoJob or KaiwoService) encountered an error and cannot proceed or recover. <code>TERMINATING</code> WorkloadStatusTerminating indicates that the workload should begin to terminate the underlying resources. <code>TERMINATED</code> WorkloadStatusTerminated indicates the workload has been terminated by the user or system. This could be due to duration deadline being met and pressure for GPU demand."},{"location":"scientist/cli/","title":"Kaiwo CLI","text":""},{"location":"scientist/cli/#installation","title":"Installation","text":"<p>The installation of Kaiwo CLI tool is easy as it's a single binary. The only requirement is a kubeconfig file to access a Kubernetes cluster (see authentication below for authentication plugins). If you are unsure where to get a kubeconfig, speak to your infrastructure/platform administrator. Just like kubectl, Kaiwo will first look for a <code>KUBECONFIG=path</code> environment variable. If <code>KUBECONFIG</code> is not set, Kaiwo will then look for kubeconfig file in the default location <code>~/.kube/config</code>.</p> <p>You can use the convenience script to install the CLI:</p> <pre><code>curl -sSL https://raw.githubusercontent.com/silogen/kaiwo/main/get-kaiwo-cli.sh | bash -s --\n</code></pre> <p>This will install the latest CLI. If you want to install a different version, you can run</p> <pre><code>KAIWO_VERSION=vX.X.X curl -sSL https://raw.githubusercontent.com/silogen/kaiwo/main/get-kaiwo-cli.sh | bash -s --\n</code></pre> <p>You're off to the races!</p> <p>If you want to uninstall the Kaiwo CLI, you can run</p> <pre><code>curl -sSL https://raw.githubusercontent.com/silogen/kaiwo/main/get-kaiwo-cli.sh | bash -s --\n</code></pre> <p>Although not strictly required, we recommend that you also install kubectl just in case you need some functionality that Kaiwo can't provide.</p>"},{"location":"scientist/cli/#authentication","title":"Authentication","text":"<p>Speak to your infrastructure/platform administrator whether your cluster requires external authentication (OIDC, Azure, GKE). If authentication is required, you will have to install a separate authentication plugin. The following plugins should work with Kaiwo, so you won't necessarily need a separate installation of kubectl.</p> <ul> <li>int128/kubelogin: kubectl plugin for Kubernetes OpenID Connect authentication (kubectl oidc-login)) (tested)</li> <li>Azure Kubelogin (untested)</li> <li>GKE: gke-gcloud-auth-plugin (tested)</li> </ul> <p>For example, your kubeconfig that uses kubelogin should resemble the following format:</p> <pre><code>apiVersion: v1\nclusters:\n-   cluster:\n        server: https://kube-api-endpoint:6443\n    name: default\ncontexts:\n-   context:\n        cluster: default\n        user: default\n    name: default\ncurrent-context: default\nkind: Config\npreferences: {}\nusers:\n-   name: default\n    user:\n        exec:\n            apiVersion: client.authentication.k8s.io/v1beta1\n            args:\n            - get-token\n            - --oidc-issuer-url=https://my.issuer.address.com/realms/realm_name\n            - --oidc-client-id=my_client_id_name\n            - --oidc-client-secret=my_client_id_secret\n            command: kubelogin\n            interactiveMode: IfAvailable\n            provideClusterInfo: false\n</code></pre> <p>In case your certificate trust store gives \"untrusted\" certificate errors, you can use <code>insecure-skip-tls-verify: true</code> under cluster and <code>--insecure-skip-tls-verify</code> in <code>kubelogin get-token</code> as a temporary workaround. As usual, we don't recommend this in production.</p>"},{"location":"scientist/cli/#configuration","title":"Configuration","text":"<p>Command-Line Flags:</p> <p>Users configure the <code>kaiwo</code> CLI tool via a YAML file. If this is not set, kaiwo cli will prompt you to create a config file interactively. The config file can be specified via the <code>--config</code> flag or the <code>KAIWOCONFIG</code> environment variable. If neither is set, kaiwo will look for a config file in the default location.</p> <p>Location Precedence:</p> <ol> <li>Path specified by <code>--config &lt;path&gt;</code> flag in <code>kaiwo submit</code>.</li> <li>Path specified by the <code>KAIWOCONFIG</code> environment variable.</li> <li>Default path: <code>~/.config/kaiwo/kaiwoconfig.yaml</code>.</li> </ol> <p>Fields:</p> <ul> <li><code>user</code>: The user's identifier (typically email) to be associated with submitted workloads (sets <code>spec.user</code> and <code>kaiwo.silogen.ai/user</code> label).</li> <li><code>clusterQueue</code>: The default Kueue <code>ClusterQueue</code> to submit workloads to (sets <code>spec.clusterQueue</code> and <code>kueue.x-k8s.io/queue-name</code> label).</li> </ul> <p>Example:</p> <pre><code># ~/.config/kaiwo/kaiwoconfig.yaml\nuser: scientist@example.com\nclusterQueue: team-a-queue\n</code></pre> <p>The <code>kaiwo submit</code> command provides an interactive prompt to create this file if it's missing and user/queue information isn't provided via flags.</p>"},{"location":"scientist/cli/#usage","title":"Usage","text":"<p>You can use the Kaiwo CLI to</p> <ul> <li><code>kaiwo submit</code>: Quickly launch Kaiwo workloads (jobs and services)</li> <li><code>kaiwo manage</code>: List and manage currently running workloads</li> <li><code>kaiwo logs</code>: Fetch workload logs</li> <li><code>kaiwo monitor</code>: Monitor (GPU) workloads</li> <li><code>kaiwo exec</code>: Execute arbitrary commands inside the workload containers</li> <li><code>kaiwo stats</code>: Check the status of your cluster</li> </ul> <p>For a list of full functionality run <code>kaiwo --help</code>, or for a specific command, <code>kaiwo &lt;command&gt; --help</code>.</p>"},{"location":"scientist/cli/#before-running-workloads-with-kaiwo","title":"Before running workloads with Kaiwo","text":"<p>Kaiwo uses Kueue to manage job queuing. Make sure your cluster-admin has created two necessary Kueue resources on the cluster: <code>ResourceFlavor</code> and <code>ClusterQueue</code>. Manifests for these can be found under <code>cluster-admins</code> directory. By default, kaiwo will always submit workloads to <code>kaiwo</code> ClusterQueue if no other queue is provided with <code>-q</code>or<code>--queue</code> option during <code>kaiwo submit</code>. Kaiwo will automatically create the namespaced <code>LocalQueue</code> resource if it doesn't exist. Speak to your cluster-admin if you are unsure which <code>ClusterQueue</code> is allocated to your team.</p>"},{"location":"scientist/cli/#submitting-workloads","title":"Submitting workloads","text":"<p>Given a Kaiwo manifest, you can submit it via the following command:</p> <pre><code>kaiwo submit -f &lt;manifest.yaml&gt;\n</code></pre> <p>As you may want to leave the user and queue definitions empty to allow different users to run the workload, you can provide this information via other methods (see above).</p> <p>Caution</p> <p>One important note about GPU requests: it is up to the user to ensure that the code can run on the requested number of GPUs. If the code is not written to run on the requested number of GPUs, the job will fail. Note that some parallelized code may only work on a specific number of GPUs such as 1, 2, 4, 8, 16, 32 but not 6, 10, 12 etc. If you are unsure, start with a single GPU and scale up as needed. For example, the total number of attention heads must be divisible by tensor parallel size.</p>"},{"location":"scientist/cli/#managing-workloads","title":"Managing workloads","text":"<p>You can list currently running workloads by running <code>kaiwo manage [flags]</code>. This displays a terminal application which you can use to:</p> <ul> <li>Select the workload type</li> <li>Delete the workload</li> <li>List the workload logs</li> <li>Port forward to the workload</li> <li>Run a command within a workload container</li> <li>Monitor a workload pod</li> </ul> <p>By default, only workloads that you have submitted are shown. You can use the following flags:</p> <ul> <li><code>-n / --namespace</code> to specify the namespace</li> <li><code>-u / --user</code> to specify a different user</li> <li><code>--all-users</code> to show workloads from all users</li> </ul>"},{"location":"scientist/cli/#fetching-workload-logs","title":"Fetching workload logs","text":"<p>You can fetch workload logs by running</p> <pre><code>kaiwo logs &lt;workloadType&gt;/&lt;workloadName&gt; [flags]\n</code></pre> <p>where <code>&lt;workloadType&gt;</code> is either <code>job</code> or <code>service</code>.</p> <p>The following flags are supported:</p> <ul> <li><code>-f / --follow</code> to follow the output</li> <li><code>-n / --namespace</code> to specify the namespace</li> <li><code>--tail</code> to specify the number of lines to tail</li> </ul>"},{"location":"scientist/cli/#monitoring-workloads","title":"Monitoring workloads","text":"<p>You can monitor workload GPU usage by running</p> <pre><code>kaiwo monitor &lt;workloadType&gt;/&lt;workloadName&gt; [flags]\n</code></pre> <p>where <code>&lt;workloadType&gt;</code> is either <code>job</code> or <code>service</code>.</p> <p>The following flags are supported:</p> <ul> <li><code>-n / --namespace</code> to specify the namespace</li> </ul>"},{"location":"scientist/cli/#executing-commands","title":"Executing commands","text":"<p>You can execute a command inside a container interactively by running</p> <pre><code>kaiwo exec &lt;workloadType&gt;/&lt;workloadName&gt; [flags]\n</code></pre> <p>where <code>&lt;workloadType&gt;</code> is either <code>job</code> or <code>service</code>.</p> <p>The following flags are supported:</p> <ul> <li><code>-i / --interactive</code> to enable interactive mode (default true)</li> <li><code>-t / --tty</code> to enable TTY (default true)</li> <li><code>--command</code> to specify the command to execute</li> <li><code>-n / --namespace</code> to specify the namespace</li> </ul>"},{"location":"scientist/cli/#checking-cluster-status","title":"Checking cluster status","text":"<p>You can check the current resource availability (including GPUs) of your cluster by running: </p> <p><pre><code>kaiwo status amd\n</code></pre> or <pre><code>kaiwo status nvidia\n</code></pre></p> <p>You can check the current queue statuses for GPU jobs by running:</p> <pre><code>kaiwo status queues\n</code></pre>"},{"location":"scientist/overview/","title":"Overview","text":"<p>This section is targeted at AI Scientists, engineers, and researchers who are interested in using Kaiwo to deploy AI workloads. It provides an overview of Kaiwo's features and benefits, as well as guidance on how to get started with deploying AI workloads with Kaiwo.</p> <p>The two main components of Kaiwo are the Kaiwo CLI and the Kaiwo Operator. The components are described here.</p> <p>Features and Benefits to AI Scientists</p> <ul> <li>Easy Deployment: Kaiwo simplifies the process of deploying AI workloads on Kubernetes, allowing scientists to focus on their research rather than infrastructure management.</li> <li>Broad Workload Support: Kaiwo supports a wide range of AI workloads, including distributed multi-node pretraining, fine-tuning, online inference, and batch inference. This flexibility allows scientists to run various types of workloads without needing to switch tools. At the moment, we support Batch Jobs, Deployments, RayJobs, and RayServices. We will continue to add support for other workload types in the future.</li> <li>Scalability: Kaiwo leverages the power of Kubernetes to scale workloads up or down based on demand, ensuring efficient resource utilization.</li> <li>Resource Management: Kaiwo provides advanced resource management capabilities, allowing scientists to allocate resources based on workload requirements. By default, Kaiwo monitors workloads' GPU usage and terminates workloads that underutilize GPUs for too long, ensuring capacity is released back to those that need it.</li> <li>Monitoring and Logging: Kaiwo offers built-in monitoring and logging features, enabling scientists to track the performance of their workloads and troubleshoot issues easily.</li> <li>Integration with Ray: Kaiwo integrates seamlessly with Ray, a powerful distributed computing framework, enabling scientists to run large-scale AI workloads efficiently.</li> <li>Integration with Kueue: Kaiwo uses Kueue for queueing and scheduling of all supported workload types, ensuring efficient management of workloads in a Kubernetes environment.</li> </ul> <p>To get started with Kaiwo, scientists can follow the quickstart guides. These guides cover various aspects of deploying AI workloads, including training, distributed training, inference, and distributed inference. The quickstart guides are designed to help scientists quickly understand how to use Kaiwo and get their workloads up and running on Kubernetes.</p>"},{"location":"scientist/quickstart/","title":"Quickstart Guides","text":"<p>This section provides quickstart guides for deploying AI workloads with Kaiwo. It covers the following topics:</p> <ul> <li>CLI quickstart</li> <li>Training</li> <li>Distributed Training</li> <li>Inference</li> <li>Distributed Inference</li> </ul>"},{"location":"scientist/quickstart/#cli-quickstart","title":"CLI Quickstart","text":"<p>This quickstart assumes Kaiwo Operator has been installed. If you suspect this is not the case, see here.</p> <p>Workloads can be submitted to Kaiwo Operator via Kaiwo CLI or Kubectl. Kaiwo CLI is a command-line interface that simplifies the process of submitting and managing workloads on Kubernetes. It provides a user-friendly interface for interacting with the Kaiwo Operator, making it easier to deploy and manage AI workloads.</p> <p>This is the TL;DR version of the installation instructions. For more details, see the installation guide.</p> <p>Make sure you have a working KUBECONFIG file. Then, install Kaiwo CLI by running the following commands in your terminal:</p> <pre><code>export KAIWO_VERSION=v.0.1 &amp;&amp; \\\nwget https://github.com/silogen/kaiwo/releases/download/$KAIWO_VERSION/kaiwo_linux_amd64 &amp;&amp; \\\nmv kaiwo_linux_amd64 kaiwo &amp;&amp; \\\nchmod +x kaiwo &amp;&amp; \\\nsudo mv kaiwo /usr/local/bin/ &amp;&amp; \\\nwget https://github.com/silogen/kaiwo/releases/download/$KAIWO_VERSION/workloads.zip &amp;&amp; \\\nunzip workloads.zip &amp;&amp; \\\nkaiwo version &amp;&amp; \\\nkaiwo help\n</code></pre> <p>You're off to the races!</p>"},{"location":"scientist/quickstart/#training","title":"Training","text":""},{"location":"scientist/quickstart/#simple-multi-gpu-training-workload","title":"Simple multi-GPU training workload","text":"<p>Let's take a look at how to deploy a multi-GPU training workload with Kaiwo. For training, we need to use a KaiwoJob. KaiwoJobs handle regular Batch Jobs and RayJobs. For example, if Kaiwo's default image is sufficient for your workload (note, the image assumes AMD GPUs), you can use the following which will run Direct Preference Optimization (DPO) on a single node with 4 GPUs. Source code and manifest can be found here:</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoJob\nmetadata:\n  name: dpo-singlenode\nspec:\n  user: test@amd.com\n  gpus: 4\n  entrypoint: |\n    accelerate launch code/dpo.py \\\n    --dataset_name trl-lib/ultrafeedback_binarized \\\n    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n    --learning_rate 5.0e-6 \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 8 \\\n    --logging_steps 25 \\\n    --eval_strategy steps \\\n    --eval_steps 50 \\\n    --output_dir Qwen2-0.5B-DPO \\\n    --no_remove_unused_columns \\\n    --use_peft \\\n    --lora_r 32 \\\n    --lora_alpha 16 \\\n    --bf16 \\\n    --optim=\"adamw_torch\"  \n  storage:\n    storageEnabled: true\n    storageClassName: multinode\n    data:\n      storageSize: 10Mi\n      mountPath: /workload\n      download:\n        git:\n        - repository: https://github.com/silogen/kaiwo.git\n          path: workloads/training/LLMs/dpo-singlenode\n          targetPath: code\n    huggingFace:\n      storageSize: \"30Gi\"\n      # automatically sets HF_HOME env variable for all containers\n      preCacheRepos:\n      - repoId: Qwen/Qwen2-0.5B-Instruct\n</code></pre> <p>You can submit this to kubernetes either via <code>kubectl apply -f filename.yaml</code> or by running <code>kaiwo submit -f filename.yaml</code>. Here are reasons why you may want to use <code>kaiwo submit</code></p> <ol> <li>Kaiwo will automatically add <code>user</code> (your email) and <code>clusterQueue</code> to the manifest upon submission. If it's your first submission, kaiwo cli will ask you for this information. You only have to do this once.</li> <li>If your kaiwojobs include <code>user</code>, you can manage and monitor your workloads easily later with <code>kaiwo manage</code>. For example, you can list your submissions, check their GPU utilization, port-forward to a pod or execute commands.</li> </ol> <p>After receiving this simple workload submission, Kaiwo Operator does the following at minimum:</p> <ol> <li>It adds your workload to the queue you provided, assuming you were granted permission to use the queue. This depends on which namespace you have access to. Queueing policies are set by administrators in KaiwoQueueConfig</li> <li>It runs your storage task (if any) and waits for it to finish before reserving GPUs. This is done by creating a separate Job that will run before your AI workload. </li> <li>Kaiwo will use binpacking and try find a node that is already partially reserved before looking for new nodes. If you didn't provide <code>gpuVendor</code> field, the operator will look for nodes with AMD GPUs. You can change this by providing <code>gpuVendor: nvidia</code>.</li> <li>Kaiwo Operator creates environment variable <code>NUM_GPUS</code> in the container, which is set to the number of GPUs requested. This means you don't have to hardcode the number of GPUs in your code. You can use this variable in your entrypoint or in your code.</li> </ol>"},{"location":"scientist/quickstart/#using-storage-task-with-kaiwojobsservices","title":"Using storage task with KaiwoJobs/Services","text":"<p>As our example above shows, Kaiwo makes it possible to download artifacts (model weights, data, code, etc.) into a persistent volume before starting the training workload. This is done by filling in <code>data</code> or <code>huggingFace</code> sections of KaiwoJob/KaiwoService. We must specify a mountPath under <code>data</code>. All subsequent targetPaths will be relative to this mountPath. The storage task will create separate persistent volume claims for <code>data</code> and <code>huggingFace</code> with the specified storageClassName and storageSize. We specify a mountPath for <code>huggingFace</code> as well, which will be used to set the HF_HOME environment variable in each container. This is useful if you want to use the HuggingFace cache in your training workload.  For more details on storage tasks, see here.</p>"},{"location":"scientist/quickstart/#distributed-training","title":"Distributed training","text":"<p>Our example of distributed training uses Ray and DeepSpeed's Zero-3 optimization which partitions optimizer states, gradients, and parameters across GPUs. This allows us to train models that are larger than the memory of a single GPU. ZeRO-3 also includes the infinity offload engine, which can offload model states to RAM or NVME disks for significant GPU memory savings. Source code and manifest can be found here.</p> <p>The training example requires object storage for saving model checkpoints. If you don't have an object storage solution, you can use MinIO, which is a self-hosted S3-compatible object storage solution. The following manifest will deploy MinIO on your cluster and create a bucket called <code>silogen-dev</code> for you. You can change the bucket name in the manifest if you want.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-secret\n  namespace: kaiwo\ndata:\n  access_key_id: bWluaW8= # minio\n  secret_key: bWluaW8xMjM= # minio123\n---\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-pvc\n  namespace: kaiwo\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 500Gi\n  storageClassName: multinode\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\n  namespace: kaiwo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: minio/minio\n        args: [\"server\", \"/data\"]\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: \"1Gi\"\n          requests:\n            cpu: \"1\"\n            memory: \"1Gi\"\n        env:\n        - name: MINIO_ROOT_USER\n          valueFrom:\n            secretKeyRef:\n              name: minio-secret\n              key: access_key_id\n        - name: MINIO_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: minio-secret\n              key: secret_key\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: minio-data\n          mountPath: /data\n      - name: bucket-init\n        image: minio/mc\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: \"1Gi\"\n          requests:\n            cpu: \"1\"\n            memory: \"1Gi\"\n        env:\n        - name: MINIO_ROOT_USER\n          valueFrom:\n            secretKeyRef:\n              name: minio-secret\n              key: access_key_id\n        - name: MINIO_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: minio-secret\n              key: secret_key\n        command:\n        - sh\n        - -c\n        - |\n          until /usr/bin/mc alias set local http://minio-service:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD; do\n            echo \"Waiting for MinIO to be available...\"\n            sleep 5\n          done\n          /usr/bin/mc mb -p local/silogen-dev || echo \"Bucket already exists\"\n          echo \"Bucket init done, sleeping forever...\"\n          tail -f /dev/null\n      volumes:\n      - name: minio-data\n        persistentVolumeClaim:\n          claimName: minio-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\n  namespace: kaiwo\nspec:\n  selector:\n    app: minio\n  ports:\n  - protocol: TCP\n    port: 9000\n    targetPort: 9000\n    name: minio-endpoint\n  type: ClusterIP\n</code></pre> <p>Once you have storage available, you can run the following KaiwoJob manifest. This will run a distributed training workload with 16 GPUs automatically using as many nodes as required. For example, if you have 4 x 8 GPU nodes where 50% of capacity is reserved on each node, Kaiwo will create 4 replicas each using 4 GPUs. Notice that this scenario of 50% GPU reservation on every node is unlikely. Kaiwo automatically uses binpacking, ensuring that each GPU node is used to maximum capacity before another GPU node is requested. </p> <p>The following manifest will also create a persistent volume claim for the model weights. The model weights will be downloaded from HuggingFace and cached in the persistent volume before any GPUs are reserved. Notice that you will need a secret that holds your Huggingface token if you are downloading a gated model.</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoJob\nmetadata:\n  name: multinode-stage-zero3-pretraining-example\nspec:\n  user: test@amd.com\n  gpus: 16\n  ray: true\n  entrypoint: |\n    python code/main.py \\\n    --model-name=meta-llama/Llama-3.1-8B-Instruct \\\n    --ds-config=./code/zero_3_offload_optim_param.json \\\n    --bucket=silogen-dev \\\n    --num-epochs=1 \\\n    --num-devices=$NUM_GPUS \\\n    --batch-size-per-device=32 \\\n    --eval-batch-size-per-device=32 \\\n    --ctx-len=1024\n  env:\n  - name: AWS_ACCESS_KEY_ID\n    valueFrom:\n      secretKeyRef:\n        name: minio-secret\n        key: access_key_id\n  - name: AWS_SECRET_ACCESS_KEY\n    valueFrom:\n      secretKeyRef:\n        name: minio-secret\n        key: secret_key\n  - name: HF_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: hf-token\n        key: hf-token\n  - name: MODEL_ID\n    value: meta-llama/Llama-3.1-8B-Instruct\n  storage:\n    storageEnabled: true\n    storageClassName: multinode\n    data:\n      storageSize: 20Mi\n      mountPath: /workload\n      download:\n        git:\n        - repository: https://github.com/silogen/kaiwo.git\n          path: workloads/training/LLMs/full-parameter-pretraining/full-param-zero3-single-multinode\n          targetPath: code\n    huggingFace:\n      storageSize: \"100Gi\"\n      mountPath: \"/hf_cache\" # Also sets the env var HF_HOME to this value in each container\n      preCacheRepos:\n      - repoId: meta-llama/Llama-3.1-8B-Instruct\n        files: []\n</code></pre> <p>Notice that our finetuning example is almost identical to this pre-training example except the former uses Lora for parameter-efficient finetuning. The only difference is the entrypoint and Lora config. You can find the manifest and source code here</p>"},{"location":"scientist/quickstart/#batch-inference-single-and-multi-node","title":"Batch inference (single- and multi-node)","text":"<p>Batch inference is a common use case for AI workloads, where large amounts of data are processed in batches to generate predictions or insights. Kaiwo supports batch inference workloads using KaiwoJobs. The following example demonstrates how to deploy a batch inference workload with Kaiwo. Our example uses vLLM and Ray to scale up inference to multiple replicas on multiple nodes (one model replica per node). </p> <p>Once again, model weights will be downloaded from HuggingFace and cached in the persistent volume before any GPUs are reserved. Notice that you will need a secret that holds your Huggingface token if you are downloading a gated model.</p> <p>Run offline inference with the following manifest. Source code and manifest can be found here</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoJob\nmetadata:\n  name: batch-inference-vllm-example\nspec:\n  user: test@amd.com\n  gpusPerReplica: 4\n  replicas: 1\n  ray: true\n  entrypoint: python code/main.py\n  env:\n  - name: HF_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: hf-token\n        key: hf-token\n  - name: MODEL_ID\n    value: meta-llama/Llama-3.1-8B-Instruct\n  storage:\n    storageEnabled: true\n    storageClassName: multinode\n    data:\n      storageSize: 20Mi\n      mountPath: /workload\n      download:\n        git:\n        - repository: https://github.com/silogen/kaiwo.git\n          path: workloads/inference/LLMs/offline-inference/vllm-batch-single-multinode\n          targetPath: code\n    huggingFace:\n      storageSize: \"100Gi\"\n      mountPath: \"/hf_cache\" # Also sets the env var HF_HOME to this value in each container\n      preCacheRepos:\n      - repoId: meta-llama/Llama-3.1-8B-Instruct\n        files: []\n</code></pre>"},{"location":"scientist/quickstart/#online-inference-single-and-multi-node","title":"Online inference (single- and multi-node)","text":"<p>Similar to our offline example, our online inference example also uses vLLM and Ray. The online inference example supports multi-node inference (one model partitioned across multiple nodes). However, we recommend sticking to a single node when possible as there are likely to be performance penalties from network bottlenecks.</p> <p>Run online inference with the following manifest. Source code and manifest can be found here</p> <pre><code>apiVersion: kaiwo.silogen.ai/v1alpha1\nkind: KaiwoService\nmetadata:\n  name: online-inference-vllm-example\nspec:\n  user: test@amd.com\n  gpus: 4\n  ray: true\n  serveConfigV2: |\n    applications:\n    - name: llm\n      route_prefix: /\n      import_path: workloads.inference.LLMs.online-inference.vllm-online-single-multinode:deployment\n      runtime_env:\n          working_dir: \"https://github.com/silogen/kaiwo/archive/b66c14303f3beadc08e11a89fd53d7f71b2a15cd.zip\"\n      deployments:\n      - name: VLLMDeployment\n        autoscaling_config:\n          metrics_interval_s: 0.2\n          look_back_period_s: 2\n          downscale_delay_s: 600\n          upscale_delay_s: 30\n          target_num_ongoing_requests_per_replica: 20\n        graceful_shutdown_timeout_s: 5\n        max_concurrent_queries: 100\n  env:\n  - name: NCCL_P2P_DISABLE\n    value: \"1\"\n  - name: MODEL_ID\n    value: \"meta-llama/Llama-3.1-8B-Instruct\"\n  - name: GPU_MEMORY_UTILIZATION\n    value: \"0.9\"\n  - name: PLACEMENT_STRATEGY\n    value: \"PACK\"\n  - name: MAX_MODEL_LEN\n    value: \"8192\"\n  - name: MAX_NUM_SEQ\n    value: \"4\"\n  - name: MAX_NUM_BATCHED_TOKENS\n    value: \"32768\"\n  - name: HF_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: hf-token\n        key: hf-token\n  storage:\n    storageEnabled: true\n    storageClassName: multinode\n    huggingFace:\n      storageSize: \"100Gi\"\n      mountPath: \"/hf_cache\" # Also sets the env var HF_HOME to this value in each container\n      preCacheRepos:\n      - repoId: meta-llama/Llama-3.1-8B-Instruct\n        files: []\n</code></pre>"},{"location":"scientist/scheduling/","title":"Scheduling","text":""},{"location":"scientist/scheduling/#replicas-gpus-gpusperreplica-and-gpuvendor","title":"<code>replicas</code>, <code>gpus</code>, <code>gpusPerReplica</code>, and <code>gpuVendor</code>","text":"<p>These fields collectively control the number of workload instances and how GPUs are allocated across them. Their interaction depends on the workload type (Job/Service) and whether Ray is used (<code>ray: true</code>).</p> <p>Purpose:</p> <ul> <li><code>replicas</code>: Sets the desired number of instances (pods). Default: 1. Ignored for non-Ray Jobs.</li> <li><code>gpus</code>: Specifies the total number of GPUs requested across all replicas. Default: 0.</li> <li><code>gpusPerReplica</code>: Specifies the number of GPUs requested per replica. Default: 0.</li> <li><code>gpuVendor</code>: Either <code>amd</code> (default) or <code>nvidia</code>. Determines the GPU resource key (e.g., <code>amd.com/gpu</code>, <code>nvidia.com/gpu</code>).</li> </ul> <p>Behavior:</p> <ol> <li> <p>Non-Ray Workloads (<code>ray: false</code>):</p> <ul> <li>KaiwoJob: Only one pod is created. <code>replicas</code> is ignored. <code>gpus</code> or <code>gpusPerReplica</code> (if set &gt; 0) determines the GPU request for the single pod's container. If both <code>gpus</code> and <code>gpusPerReplica</code> are set, <code>gpusPerReplica</code> takes precedence if &gt; 0, otherwise <code>gpus</code> is used.</li> <li>KaiwoService (Deployment): <code>replicas</code> directly sets the <code>deployment.spec.replicas</code>. <code>gpus</code> or <code>gpusPerReplica</code> (if set &gt; 0) determines the GPU request for each replica's container. If both <code>gpus</code> and <code>gpusPerReplica</code> are set, <code>gpusPerReplica</code> takes precedence if &gt; 0, otherwise <code>gpus</code> is used (implying <code>gpusPerReplica = gpus / replicas</code>, though this division isn't explicitly performed; the request per pod is set based on the determined <code>gpusPerReplica</code> value).</li> </ul> </li> <li> <p>Ray Workloads (<code>ray: true</code>):</p> <ul> <li>The controller performs a calculation (<code>CalculateNumberOfReplicas</code>) considering cluster node capacity (specifically, the minimum GPU capacity available on nodes matching the <code>gpuVendor</code>, referred to as <code>minGpusPerNode</code>).</li> <li>User Precedence: If the user explicitly sets both <code>replicas</code> (&gt; 0) and <code>gpusPerReplica</code> (&gt; 0), these values are used directly, provided the total requested GPUs (<code>replicas * gpusPerReplica</code>) does not exceed the total available GPUs of the specified <code>gpuVendor</code> in the cluster. The <code>gpus</code> field is ignored in this case.</li> <li>Calculation Fallback: If the user does not explicitly set both <code>replicas</code> and <code>gpusPerReplica</code>, or if the requested total exceeds cluster capacity, the controller calculates the optimal <code>replicas</code> and <code>gpusPerReplica</code> based on the <code>gpus</code> field and the cluster's <code>minGpusPerNode</code>.<ul> <li>The <code>totalUserRequestedGpus</code> is determined (using <code>gpus</code> field, capped at total cluster capacity).</li> <li>The final <code>replicas</code> is calculated as <code>ceil(totalUserRequestedGpus / minGpusPerNode)</code>.</li> <li>The final <code>gpusPerReplica</code> is calculated as <code>totalUserRequestedGpus / replicas</code>.</li> </ul> </li> <li>The calculated or user-provided <code>replicas</code> value sets the Ray worker group replica count (<code>minReplicas</code>, <code>maxReplicas</code>, <code>replicas</code>). This is due the fact that Kueue does not support Ray's autoscaling.</li> <li>The calculated or user-provided <code>gpusPerReplica</code> value sets the GPU resource request/limit for each Ray worker pod's container.</li> </ul> </li> </ol> <p>Summary Table (Ray Workloads):</p> User Input (<code>spec.*</code>) Calculation Performed? Outcome (<code>replicas</code>, <code>gpusPerReplica</code>) Notes <code>replicas &gt; 0</code>, <code>gpusPerReplica &gt; 0</code> No* Uses user's <code>replicas</code>, user's <code>gpusPerReplica</code> *If total fits cluster. <code>gpus</code> ignored. Highest precedence. <code>gpus &gt; 0</code> (only) Yes Calculated based on <code>gpus</code> and <code>minGpusPerNode</code> Aims to maximize GPUs per node up to <code>minGpusPerNode</code>. <code>replicas &gt; 0</code>, <code>gpus &gt; 0</code> Yes Calculated based on <code>gpus</code> and <code>minGpusPerNode</code> (user <code>replicas</code> ignored) Falls back to calculation based on total <code>gpus</code>. <code>gpusPerReplica &gt; 0</code>, <code>gpus &gt; 0</code> Yes Calculated based on <code>gpus</code> and <code>minGpusPerNode</code> (user <code>gpusPerReplica</code> ignored) Falls back to calculation based on total <code>gpus</code>. All three set No* Uses user's <code>replicas</code>, user's <code>gpusPerReplica</code> *If total fits cluster (like row 1). Otherwise, calculates based on <code>gpus</code>. None set (or only <code>gpuVendor</code>) No <code>replicas=1</code>, <code>gpusPerReplica=0</code> No GPUs requested."}]}