apiVersion: batch/v1
kind: Job
metadata:
  name: amd-gpu-test
  namespace: default
spec:
  template:
    spec:
      containers:
      - name: gpu-test
        image: rocm/pytorch:latest
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "Testing AMD GPU access..."
          python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU count: {torch.cuda.device_count()}')
              for i in range(torch.cuda.device_count()):
                  print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
          else:
              print('No CUDA/ROCm GPU available')
          "
        resources:
          limits:
            amd.com/gpu: 1
          requests:
            amd.com/gpu: 1
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1000
      restartPolicy: Never
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
  backoffLimit: 3
