apiVersion: batch/v1
kind: Job
metadata:
  name: test-gpu-job
  labels:
    app: gpu-test
spec:
  template:
    metadata:
      labels:
        app: gpu-test
    spec:
      containers:
      - name: gpu-test
        image: rocm/pytorch:latest
        command: ["python", "-c", "
import torch
import os
print('=== AMD GPU Test ===')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU count: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i} name: {torch.cuda.get_device_name(i)}')
        print(f'GPU {i} memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB')
    # Test basic GPU operation
    x = torch.randn(1000, 1000).cuda()
    y = torch.randn(1000, 1000).cuda()
    z = torch.mm(x, y)
    print(f'GPU computation test: {z.shape}')
    print('✅ GPU test successful!')
else:
    print('❌ No GPU available')
print('=== Environment Info ===')
print(f'ROCm version: {os.environ.get(\"ROCm_VERSION\", \"Not set\")}')
print(f'GPU devices: {os.environ.get(\"GPU_DEVICE_ORDINAL\", \"Not set\")}')
"]
        resources:
          limits:
            amd.com/gpu: 1
          requests:
            amd.com/gpu: 1
        env:
        - name: ROCm_VERSION
          value: "5.7"
        - name: GPU_DEVICE_ORDINAL
          value: "0"
      restartPolicy: Never
      nodeSelector:
        amd.com/gpu: "present"
  backoffLimit: 4
