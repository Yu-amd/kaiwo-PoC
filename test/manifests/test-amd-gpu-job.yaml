apiVersion: batch/v1
kind: Job
metadata:
  name: amd-gpu-test
  namespace: default
spec:
  template:
    spec:
      containers:
      - name: gpu-test
        image: rocm/pytorch:latest
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "=== AMD ROCm GPU Test ==="
          echo "Testing AMD Instinct GPU access..."
          
          echo ""
          echo "1. Checking AMD GPU devices..."
          ls -la /dev/dri/ | grep -E "(card|renderD)"
          ls -la /dev/kfd
          
          echo ""
          echo "2. Checking ROCm environment..."
          echo "ROCm version: $(rocm-smi --version 2>/dev/null | head -1 || echo 'Not available')"
          echo "HIP version: $(hipconfig | grep 'HIP version' || echo 'Not available')"
          
          echo ""
          echo "3. Testing AMD GPU with PyTorch..."
          python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'ROCm available: {torch.cuda.is_available()}')
          print(f'AMD GPU count: {torch.cuda.device_count()}')
          
          if torch.cuda.is_available():
              print('✅ AMD GPU is available!')
              for i in range(torch.cuda.device_count()):
                  gpu_name = torch.cuda.get_device_name(i)
                  props = torch.cuda.get_device_properties(i)
                  print(f'AMD GPU {i}: {gpu_name}')
                  print(f'  - Memory: {props.total_memory / 1024**3:.1f} GB')
                  print(f'  - Compute Capability: {props.major}.{props.minor}')
              
              # Test AMD GPU computation
              try:
                  x = torch.randn(100, 100).cuda()
                  y = torch.randn(100, 100).cuda()
                  z = torch.mm(x, y)
                  print(f'✅ AMD GPU computation test successful: {z.shape}')
                  print('✅ AMD ROCm GPU is working correctly!')
              except Exception as e:
                  print(f'❌ AMD GPU computation test failed: {e}')
          else:
              print('❌ No AMD GPU available to PyTorch')
              print('This could mean:')
              print('- AMD GPU device plugin not working')
              print('- Container not getting GPU access')
              print('- ROCm not properly configured')
          "
          
          echo ""
          echo "4. Testing AMD GPU with rocm-smi..."
          rocm-smi --showproductname || echo "rocm-smi not available"
          
          echo ""
          echo "=== AMD ROCm GPU Test Complete ==="
        resources:
          limits:
            amd.com/gpu: 1
          requests:
            amd.com/gpu: 1
        volumeMounts:
        - name: dri
          mountPath: /dev/dri
        - name: kfd
          mountPath: /dev/kfd
      restartPolicy: Never
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      volumes:
      - name: dri
        hostPath:
          path: /dev/dri
      - name: kfd
        hostPath:
          path: /dev/kfd
  backoffLimit: 3
