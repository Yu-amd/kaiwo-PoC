apiVersion: batch/v1
kind: Job
metadata:
  name: amd-gpu-detailed-test
  namespace: default
spec:
  template:
    spec:
      containers:
      - name: gpu-test
        image: rocm/pytorch:latest
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "=== AMD GPU Detailed Test ==="
          echo "1. Checking system GPU devices..."
          ls -la /dev/dri/ 2>/dev/null || echo "No /dev/dri devices found"
          ls -la /dev/kfd 2>/dev/null || echo "No /dev/kfd device found"
          
          echo ""
          echo "2. Checking environment variables..."
          echo "ROCm_VERSION: ${ROCm_VERSION:-Not set}"
          echo "GPU_DEVICE_ORDINAL: ${GPU_DEVICE_ORDINAL:-Not set}"
          echo "HIP_VISIBLE_DEVICES: ${HIP_VISIBLE_DEVICES:-Not set}"
          
          echo ""
          echo "3. Checking ROCm tools..."
          which rocm-smi 2>/dev/null && echo "rocm-smi found" || echo "rocm-smi not found"
          which hipconfig 2>/dev/null && echo "hipconfig found" || echo "hipconfig not found"
          
          echo ""
          echo "4. Testing PyTorch with ROCm..."
          python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'PyTorch build info: {torch.version.cuda if hasattr(torch.version, \"cuda\") else \"No CUDA info\"}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          print(f'CUDA device count: {torch.cuda.device_count()}')
          
          if torch.cuda.is_available():
              print('✅ GPU is available!')
              for i in range(torch.cuda.device_count()):
                  print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
                  props = torch.cuda.get_device_properties(i)
                  print(f'  - Memory: {props.total_memory / 1024**3:.1f} GB')
                  print(f'  - Compute Capability: {props.major}.{props.minor}')
              
              # Test basic GPU operation
              try:
                  x = torch.randn(100, 100).cuda()
                  y = torch.randn(100, 100).cuda()
                  z = torch.mm(x, y)
                  print(f'✅ GPU computation test successful: {z.shape}')
              except Exception as e:
                  print(f'❌ GPU computation test failed: {e}')
          else:
              print('❌ No GPU available to PyTorch')
              print('This could mean:')
              print('- GPU device plugin not working')
              print('- Container not getting GPU access')
              print('- ROCm not properly configured')
          "
          
          echo ""
          echo "5. Checking container capabilities..."
          cat /proc/1/status | grep CapEff || echo "Cannot read capabilities"
          
          echo ""
          echo "6. Checking mounted devices..."
          mount | grep -E "(dri|kfd)" || echo "No GPU-related mounts found"
          
          echo ""
          echo "=== Test Complete ==="
        resources:
          limits:
            amd.com/gpu: 1
          requests:
            amd.com/gpu: 1
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - name: dri
          mountPath: /dev/dri
        - name: kfd
          mountPath: /dev/kfd
      restartPolicy: Never
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      volumes:
      - name: dri
        hostPath:
          path: /dev/dri
      - name: kfd
        hostPath:
          path: /dev/kfd
  backoffLimit: 3
